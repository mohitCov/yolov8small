{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQmIR21EPmTc",
        "outputId": "be850f21-f16d-4a4b-8d25-b78fc96b4b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[38;5;39m2025-06-07 07:16:25 - INFO - Logger initialized\u001b[0m\n",
            "INFO:YOLOComparison:Logger initialized\n"
          ]
        }
      ],
      "source": [
        "# 1. Logging Setup (run first)\n",
        "\n",
        "import logging\n",
        "\n",
        "class CustomFormatter(logging.Formatter):\n",
        "    \"\"\"Custom formatter with colors for notebook output\"\"\"\n",
        "    grey = \"\\x1b[38;21m\"\n",
        "    blue = \"\\x1b[38;5;39m\"\n",
        "    yellow = \"\\x1b[38;5;226m\"\n",
        "    red = \"\\x1b[38;5;196m\"\n",
        "    reset = \"\\x1b[0m\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "                         datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
        "        self.FORMATS = {\n",
        "            logging.DEBUG: self.grey,\n",
        "            logging.INFO: self.blue,\n",
        "            logging.WARNING: self.yellow,\n",
        "            logging.ERROR: self.red\n",
        "        }\n",
        "\n",
        "    def format(self, record):\n",
        "        color = self.FORMATS.get(record.levelno, self.grey)\n",
        "        log_fmt = f\"{color}%(asctime)s - %(levelname)s - %(message)s{self.reset}\"\n",
        "        formatter = logging.Formatter(log_fmt, datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
        "        return formatter.format(record)\n",
        "\n",
        "# Setup logger\n",
        "logger = logging.getLogger(\"YOLOComparison\")\n",
        "logger.setLevel(logging.INFO)\n",
        "if not logger.handlers:\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setFormatter(CustomFormatter())\n",
        "    logger.addHandler(console_handler)\n",
        "logger.info(\"Logger initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all required packages (run this first if you get ModuleNotFoundError)\n",
        "!pip install ultralytics sahi pycocotools pandas seaborn --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ma-uyddPQD93",
        "outputId": "d595f4b1-7d6c-4fd5-f714-2e0818a0b3fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Imports & Config (run after logging setup)\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import matplotlib.patches as patches\n",
        "from ultralytics import YOLO, __version__ as yolo_version\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Google Drive mounting (run only in Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path configs (Change if needed)\n",
        "BASE_DIR = '/content/drive/MyDrive/new scope model'\n",
        "DATA_YAML = f'{BASE_DIR}/data.yaml'\n",
        "DATASET_DIR = '/content/drive/MyDrive/Samplesmall_dataset'\n",
        "\n",
        "logger.info(f\"Using Ultralytics YOLO version: {yolo_version}\")\n",
        "for path in [BASE_DIR, DATA_YAML, DATASET_DIR]:\n",
        "    if not os.path.exists(path):\n",
        "        logger.error(f\"Path not found: {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATPBYsIoP6DB",
        "outputId": "52b0f0b8-d7f6-4fd2-a969-a88b53e618e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[38;5;39m2025-06-07 07:28:11 - INFO - Using Ultralytics YOLO version: 8.3.151\u001b[0m\n",
            "INFO:YOLOComparison:Using Ultralytics YOLO version: 8.3.151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def plot_comparison_grid(images, detections_list, class_names, method_names, save_path):\n",
        "    \"\"\"\n",
        "    images: list of np.array images [img1, img2]\n",
        "    detections_list: list of lists, shape [n_methods][n_images][detections]\n",
        "    class_names: list of str\n",
        "    method_names: list of str, e.g. ['YOLOv8', 'Enhanced YOLO', 'SAHI']\n",
        "    save_path: where to save the output image\n",
        "    \"\"\"\n",
        "    n_images = len(images)\n",
        "    n_methods = len(method_names)\n",
        "    fig, axes = plt.subplots(n_images, n_methods, figsize=(5 * n_methods, 5 * n_images))\n",
        "\n",
        "    for i in range(n_images):\n",
        "        for j in range(n_methods):\n",
        "            ax = axes[i, j] if n_images > 1 else axes[j]\n",
        "            ax.imshow(images[i])\n",
        "            # Draw detections for this method/image\n",
        "            for det in detections_list[j][i]:\n",
        "                bbox = det['bbox']\n",
        "                label = class_names[det['class_id']]\n",
        "                conf = det['confidence']\n",
        "                rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3],\n",
        "                                         linewidth=2, edgecolor='r', facecolor='none')\n",
        "                ax.add_patch(rect)\n",
        "                ax.text(bbox[0], bbox[1]-5, f'{label}: {conf:.2f}',\n",
        "                        color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
        "            if i == 0:\n",
        "                ax.set_title(method_names[j])\n",
        "            ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"[INFO] Saved comparison grid to {save_path}\")"
      ],
      "metadata": {
        "id": "YitjVznERKu5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelEvaluator:\n",
        "    def __init__(self, base_dir, data_yaml):\n",
        "        self.base_dir = base_dir\n",
        "        self.data_yaml = data_yaml\n",
        "        self.visualizer = ResultsVisualizer(base_dir)\n",
        "\n",
        "        # Load class names from yaml\n",
        "        with open(data_yaml, 'r') as f:\n",
        "            self.data_config = yaml.safe_load(f)\n",
        "        self.class_names = self.data_config['names']\n",
        "\n",
        "        logger.info(f\"Initialized evaluator with {len(self.class_names)} classes\")\n",
        "\n",
        "    def train_and_evaluate_baseline(self):\n",
        "        \"\"\"Train and evaluate baseline YOLOv8 model\"\"\"\n",
        "        logger.info(\"Starting baseline model training...\")\n",
        "\n",
        "        model = YOLO('yolov8n.pt')\n",
        "        results = model.train(\n",
        "            data=self.data_yaml,\n",
        "            epochs=15,\n",
        "            imgsz=640,\n",
        "            project=self.base_dir,\n",
        "            name='baseline_model'\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        val_results = model.val(data=self.data_yaml)\n",
        "        return model, val_results\n",
        "\n",
        "    def train_and_evaluate_enhanced(self):\n",
        "        \"\"\"Train and evaluate enhanced model (larger size + TTA)\"\"\"\n",
        "        logger.info(\"Starting enhanced model training...\")\n",
        "\n",
        "        model = YOLO('yolov8n.pt')\n",
        "        results = model.train(\n",
        "            data=self.data_yaml,\n",
        "            epochs=15,\n",
        "            imgsz=1024,\n",
        "            project=self.base_dir,\n",
        "            name='enhanced_model'\n",
        "        )\n",
        "\n",
        "        # Evaluate with TTA\n",
        "        val_results = model.val(\n",
        "            data=self.data_yaml,\n",
        "            imgsz=1024,\n",
        "            augment=True\n",
        "        )\n",
        "        return model, val_results\n",
        "\n",
        "    def evaluate_with_sahi(self, model_path):\n",
        "        \"\"\"Evaluate using SAHI with better error handling\"\"\"\n",
        "        try:\n",
        "            from sahi import AutoDetectionModel\n",
        "            from sahi.predict import get_sliced_prediction\n",
        "\n",
        "            logger.info(f\"Loading model from {model_path}\")\n",
        "            if not os.path.exists(model_path):\n",
        "                raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "\n",
        "            detection_model = AutoDetectionModel.from_pretrained(\n",
        "                model_type='ultralytics',\n",
        "                model_path=model_path,\n",
        "                confidence_threshold=0.3,\n",
        "                device='cuda'\n",
        "            )\n",
        "\n",
        "            test_images_dir = os.path.join(DATASET_DIR, 'test/images')\n",
        "            if not os.path.exists(test_images_dir):\n",
        "                raise FileNotFoundError(f\"Test images dir not found: {test_images_dir}\")\n",
        "\n",
        "            results = []\n",
        "            for image_name in os.listdir(test_images_dir):\n",
        "                if image_name.endswith(('.jpg', '.png')):\n",
        "                    image_path = os.path.join(test_images_dir, image_name)\n",
        "                    try:\n",
        "                        result = get_sliced_prediction(\n",
        "                            image=image_path,\n",
        "                            detection_model=detection_model,\n",
        "                            slice_height=512,\n",
        "                            slice_width=512,\n",
        "                            overlap_height_ratio=0.2,\n",
        "                            overlap_width_ratio=0.2\n",
        "                        )\n",
        "                        results.append(result)\n",
        "                        logger.debug(f\"Processed {image_name} successfully\")\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Failed to process {image_name}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"SAHI evaluation failed: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def run_complete_evaluation(self):\n",
        "        \"\"\"Run complete evaluation pipeline\"\"\"\n",
        "        try:\n",
        "            # 1. Baseline evaluation\n",
        "            baseline_model, baseline_results = self.train_and_evaluate_baseline()\n",
        "\n",
        "            # 2. Enhanced evaluation\n",
        "            enhanced_model, enhanced_results = self.train_and_evaluate_enhanced()\n",
        "\n",
        "            # 3. SAHI evaluation\n",
        "            sahi_results = self.evaluate_with_sahi(\n",
        "                f'{self.base_dir}/enhanced_model/weights/best.pt'\n",
        "            )\n",
        "\n",
        "            # Function for safe metric extraction\n",
        "            def safe_get_metrics(results):\n",
        "                \"\"\"Safe metric extraction with fallbacks\"\"\"\n",
        "                if not hasattr(results, 'box'):\n",
        "                    logger.error(\"Validation results missing 'box' attribute\")\n",
        "                    return {\n",
        "                        'mAP50': 0,\n",
        "                        'mAP50-95': 0,\n",
        "                        'recall': 0,\n",
        "                        'precision': 0\n",
        "                    }\n",
        "\n",
        "                box = results.box\n",
        "                return {\n",
        "                    'mAP50': getattr(box, 'map50', 0),\n",
        "                    'mAP50-95': getattr(box, 'map', 0),\n",
        "                    'recall': getattr(box, 'r', 0),\n",
        "                    'precision': getattr(box, 'p', 0)\n",
        "                }\n",
        "\n",
        "            # 4. Collect all results\n",
        "            baseline_metrics = safe_get_metrics(baseline_results)\n",
        "            enhanced_metrics = safe_get_metrics(enhanced_results)\n",
        "\n",
        "            results_dict = {\n",
        "                'summary_metrics': {\n",
        "                    'Baseline': {\n",
        "                        'mAP50': baseline_metrics['mAP50'],\n",
        "                        'mAP50-95': baseline_metrics['mAP50-95'],\n",
        "                        'recall': baseline_metrics['recall']\n",
        "                    },\n",
        "                    'Enhanced+TTA': {\n",
        "                        'mAP50': enhanced_metrics['mAP50'],\n",
        "                        'mAP50-95': enhanced_metrics['mAP50-95'],\n",
        "                        'recall': enhanced_metrics['recall']\n",
        "                    }\n",
        "                },\n",
        "                'confusion_matrices': {\n",
        "    'Baseline': {\n",
        "        'true': [0]*len(self.class_names),\n",
        "        'pred': [0]*len(self.class_names)\n",
        "    },\n",
        "    'Enhanced+TTA': {\n",
        "        'true': [0]*len(self.class_names),\n",
        "        'pred': [0]*len(self.class_names)\n",
        "    }\n",
        "}\n",
        "            }\n",
        "\n",
        "            # 5. Generate visualizations\n",
        "            self.visualizer.generate_all_visualizations(\n",
        "                results_dict,\n",
        "                self.class_names\n",
        "            )\n",
        "\n",
        "            logger.info(\"Evaluation completed successfully!\")\n",
        "            return results_dict\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during evaluation: {str(e)}\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    def test_metrics_extraction(self):\n",
        "        \"\"\"Test metrics extraction works with current YOLO version\"\"\"\n",
        "        from unittest.mock import MagicMock\n",
        "\n",
        "        # Create mock results object\n",
        "        mock_results = MagicMock()\n",
        "        mock_results.box = MagicMock()\n",
        "        mock_results.box.map50 = 0.5\n",
        "        mock_results.box.map = 0.4\n",
        "        mock_results.box.r = 0.3\n",
        "        mock_results.box.p = 0.6\n",
        "        mock_results.confusion_matrix = None\n",
        "\n",
        "        # Since safe_get_metrics is inside run_complete_evaluation, redefining it here\n",
        "        def safe_get_metrics(results):\n",
        "            if not hasattr(results, 'box'):\n",
        "                return {\n",
        "                    'mAP50': 0,\n",
        "                    'mAP50-95': 0,\n",
        "                    'recall': 0,\n",
        "                    'precision': 0\n",
        "                }\n",
        "\n",
        "            box = results.box\n",
        "            return {\n",
        "                'mAP50': getattr(box, 'map50', 0),\n",
        "                'mAP50-95': getattr(box, 'map', 0),\n",
        "                'recall': getattr(box, 'r', 0),\n",
        "                'precision': getattr(box, 'p', 0)\n",
        "            }\n",
        "\n",
        "        metrics = safe_get_metrics(mock_results)\n",
        "        assert metrics['mAP50'] == 0.5\n",
        "        assert metrics['recall'] == 0.3\n",
        "        logger.info(\"✅ Metrics extraction test passed!\")\n",
        "\n",
        "        # Test missing attribute handling\n",
        "        mock_results.box = None\n",
        "        metrics = safe_get_metrics(mock_results)\n",
        "        assert metrics['mAP50'] == 0\n",
        "        logger.info(\"✅ Error handling test passed!\")\n"
      ],
      "metadata": {
        "id": "scgNqIKyQisn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. ModelEvaluator Class\n",
        "\n",
        "class ModelEvaluator:\n",
        "    def __init__(self, base_dir, data_yaml, dataset_dir):\n",
        "        self.base_dir = base_dir\n",
        "        self.data_yaml = data_yaml\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.visualizer = ResultsVisualizer(base_dir)\n",
        "        with open(data_yaml, 'r') as f:\n",
        "            self.data_config = yaml.safe_load(f)\n",
        "        self.class_names = self.data_config['names']\n",
        "        logger.info(f\"Initialized evaluator with {len(self.class_names)} classes\")\n",
        "\n",
        "    def train_and_evaluate_baseline(self):\n",
        "        logger.info(\"Starting baseline model training...\")\n",
        "        model = YOLO('yolov8n.pt')\n",
        "        results = model.train(\n",
        "            data=self.data_yaml,\n",
        "            epochs=2,\n",
        "            imgsz=640,\n",
        "            project=self.base_dir,\n",
        "            name='baseline_model'\n",
        "        )\n",
        "        val_results = model.val(data=self.data_yaml)\n",
        "        return model, val_results\n",
        "\n",
        "    def train_and_evaluate_enhanced(self):\n",
        "        logger.info(\"Starting enhanced model training...\")\n",
        "        model = YOLO('yolov8n.pt')\n",
        "        results = model.train(\n",
        "            data=self.data_yaml,\n",
        "            epochs=2,\n",
        "            imgsz=1024,\n",
        "            project=self.base_dir,\n",
        "            name='enhanced_model'\n",
        "        )\n",
        "        val_results = model.val(\n",
        "            data=self.data_yaml,\n",
        "            imgsz=1024,\n",
        "            augment=True\n",
        "        )\n",
        "        return model, val_results\n",
        "\n",
        "    def evaluate_with_sahi(self, model_path):\n",
        "        try:\n",
        "            from sahi import AutoDetectionModel\n",
        "            from sahi.predict import get_sliced_prediction\n",
        "            logger.info(f\"Loading model from {model_path}\")\n",
        "            if not os.path.exists(model_path):\n",
        "                raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "            detection_model = AutoDetectionModel.from_pretrained(\n",
        "                model_type='ultralytics',\n",
        "                model_path=model_path,\n",
        "                confidence_threshold=0.3,\n",
        "                device='cuda'\n",
        "            )\n",
        "            test_images_dir = os.path.join(self.dataset_dir, 'test/images')\n",
        "            if not os.path.exists(test_images_dir):\n",
        "                raise FileNotFoundError(f\"Test images dir not found: {test_images_dir}\")\n",
        "            results = []\n",
        "            for image_name in os.listdir(test_images_dir):\n",
        "                if image_name.endswith(('.jpg', '.png')):\n",
        "                    image_path = os.path.join(test_images_dir, image_name)\n",
        "                    try:\n",
        "                        result = get_sliced_prediction(\n",
        "                            image=image_path,\n",
        "                            detection_model=detection_model,\n",
        "                            slice_height=512,\n",
        "                            slice_width=512,\n",
        "                            overlap_height_ratio=0.2,\n",
        "                            overlap_width_ratio=0.2\n",
        "                        )\n",
        "                        results.append(result)\n",
        "                        logger.debug(f\"Processed {image_name} successfully\")\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Failed to process {image_name}: {str(e)}\")\n",
        "                        continue\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            logger.error(f\"SAHI evaluation failed: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def run_complete_evaluation(self):\n",
        "        try:\n",
        "            # 1. Baseline evaluation\n",
        "            baseline_model, baseline_results = self.train_and_evaluate_baseline()\n",
        "            # 2. Enhanced evaluation\n",
        "            enhanced_model, enhanced_results = self.train_and_evaluate_enhanced()\n",
        "            # 3. SAHI evaluation\n",
        "            sahi_results = self.evaluate_with_sahi(\n",
        "                f'{self.base_dir}/enhanced_model/weights/best.pt'\n",
        "            )\n",
        "            # 4. Collect all results\n",
        "            def safe_get_metrics(results):\n",
        "                if not hasattr(results, 'box'):\n",
        "                    logger.error(\"Validation results missing 'box' attribute\")\n",
        "                    return {\n",
        "                        'mAP50': 0,\n",
        "                        'mAP50-95': 0,\n",
        "                        'recall': 0,\n",
        "                        'precision': 0\n",
        "                    }\n",
        "                box = results.box\n",
        "                return {\n",
        "                    'mAP50': getattr(box, 'map50', 0),\n",
        "                    'mAP50-95': getattr(box, 'map', 0),\n",
        "                    'recall': getattr(box, 'r', 0),\n",
        "                    'precision': getattr(box, 'p', 0)\n",
        "                }\n",
        "            baseline_metrics = safe_get_metrics(baseline_results)\n",
        "            enhanced_metrics = safe_get_metrics(enhanced_results)\n",
        "            results_dict = {\n",
        "                'summary_metrics': {\n",
        "                    'Baseline': {\n",
        "                        'mAP50': baseline_metrics['mAP50'],\n",
        "                        'mAP50-95': baseline_metrics['mAP50-95'],\n",
        "                        'recall': baseline_metrics['recall']\n",
        "                    },\n",
        "                    'Enhanced+TTA': {\n",
        "                        'mAP50': enhanced_metrics['mAP50'],\n",
        "                        'mAP50-95': enhanced_metrics['mAP50-95'],\n",
        "                        'recall': enhanced_metrics['recall']\n",
        "                    }\n",
        "                },\n",
        "                'confusion_matrices': {\n",
        "                    'Baseline': {\n",
        "                        'true': getattr(baseline_results.box.confusion_matrix, 'matrix', np.zeros((len(self.class_names), len(self.class_names)))).T,\n",
        "                        'pred': list(range(len(self.class_names)))\n",
        "                    },\n",
        "                    'Enhanced+TTA': {\n",
        "                        'true': getattr(enhanced_results.box.confusion_matrix, 'matrix', np.zeros((len(self.class_names), len(self.class_names)))).T,\n",
        "                        'pred': list(range(len(self.class_names)))\n",
        "                    }\n",
        "                },\n",
        "                'precision': [\n",
        "                    getattr(baseline_results.box, 'p', []),\n",
        "                    getattr(enhanced_results.box, 'p', [])\n",
        "                ],\n",
        "                'recall': [\n",
        "                    getattr(baseline_results.box, 'r', []),\n",
        "                    getattr(enhanced_results.box, 'r', [])\n",
        "                ]\n",
        "            }\n",
        "            self.visualizer.generate_all_visualizations(\n",
        "                results_dict,\n",
        "                self.class_names\n",
        "            )\n",
        "            logger.info(\"Evaluation completed successfully!\")\n",
        "            return results_dict\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during evaluation: {str(e)}\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    def test_metrics_extraction(self):\n",
        "        from unittest.mock import MagicMock\n",
        "        mock_results = MagicMock()\n",
        "        mock_results.box = MagicMock()\n",
        "        mock_results.box.map50 = 0.5\n",
        "        mock_results.box.map = 0.4\n",
        "        mock_results.box.r = 0.3\n",
        "        mock_results.box.p = 0.6\n",
        "        mock_results.confusion_matrix = None\n",
        "        def safe_get_metrics(results):\n",
        "            if not hasattr(results, 'box'):\n",
        "                return {'mAP50': 0, 'mAP50-95': 0, 'recall': 0, 'precision': 0}\n",
        "            box = results.box\n",
        "            return {\n",
        "                'mAP50': getattr(box, 'map50', 0),\n",
        "                'mAP50-95': getattr(box, 'map', 0),\n",
        "                'recall': getattr(box, 'r', 0),\n",
        "                'precision': getattr(box, 'p', 0)\n",
        "            }\n",
        "        metrics = safe_get_metrics(mock_results)\n",
        "        assert metrics['mAP50'] == 0.5\n",
        "        assert metrics['recall'] == 0.3\n",
        "        logger.info(\"✅ Metrics extraction test passed!\")\n",
        "        mock_results.box = None\n",
        "        metrics = safe_get_metrics(mock_results)\n",
        "        assert metrics['mAP50'] == 0\n",
        "        logger.info(\"✅ Error handling test passed!\")"
      ],
      "metadata": {
        "id": "SbtaCT3WQoA7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Main Pipeline\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = ModelEvaluator(BASE_DIR, DATA_YAML, DATASET_DIR)\n",
        "evaluator.test_metrics_extraction()\n",
        "\n",
        "# Run evaluation\n",
        "results = evaluator.run_complete_evaluation()\n",
        "\n",
        "logger.info(f\"Results saved in: {evaluator.visualizer.results_dir}\")"
      ],
      "metadata": {
        "id": "Vd6Z6HEmQtUY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}