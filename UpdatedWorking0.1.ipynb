{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQmIR21EPmTc",
        "outputId": "41a86ff0-eab8-4e07-ca9d-398274c9788d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[38;5;39m2025-06-07 11:19:56 - INFO - Logger initialized\u001b[0m\n",
            "INFO:YOLOComparison:Logger initialized\n"
          ]
        }
      ],
      "source": [
        "# 1. Logging Setup (run first)\n",
        "\n",
        "import logging\n",
        "\n",
        "class CustomFormatter(logging.Formatter):\n",
        "    \"\"\"Custom formatter with colors for notebook output\"\"\"\n",
        "    grey = \"\\x1b[38;21m\"\n",
        "    blue = \"\\x1b[38;5;39m\"\n",
        "    yellow = \"\\x1b[38;5;226m\"\n",
        "    red = \"\\x1b[38;5;196m\"\n",
        "    reset = \"\\x1b[0m\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "                         datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
        "        self.FORMATS = {\n",
        "            logging.DEBUG: self.grey,\n",
        "            logging.INFO: self.blue,\n",
        "            logging.WARNING: self.yellow,\n",
        "            logging.ERROR: self.red\n",
        "        }\n",
        "\n",
        "    def format(self, record):\n",
        "        color = self.FORMATS.get(record.levelno, self.grey)\n",
        "        log_fmt = f\"{color}%(asctime)s - %(levelname)s - %(message)s{self.reset}\"\n",
        "        formatter = logging.Formatter(log_fmt, datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
        "        return formatter.format(record)\n",
        "\n",
        "# Setup logger\n",
        "logger = logging.getLogger(\"YOLOComparison\")\n",
        "logger.setLevel(logging.INFO)\n",
        "if not logger.handlers:\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setFormatter(CustomFormatter())\n",
        "    logger.addHandler(console_handler)\n",
        "logger.info(\"Logger initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all required packages (run this first if you get ModuleNotFoundError)\n",
        "!pip install ultralytics sahi pycocotools pandas seaborn --quiet"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ma-uyddPQD93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Imports & Config (run after logging setup)\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import matplotlib.patches as patches\n",
        "from ultralytics import YOLO, __version__ as yolo_version\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Google Drive mounting (run only in Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path configs (Change if needed)\n",
        "BASE_DIR = '/content/drive/MyDrive/new scope model'\n",
        "DATA_YAML = f'{BASE_DIR}/data.yaml'\n",
        "DATASET_DIR = '/content/drive/MyDrive/Samplesmall_dataset'\n",
        "\n",
        "logger.info(f\"Using Ultralytics YOLO version: {yolo_version}\")\n",
        "for path in [BASE_DIR, DATA_YAML, DATASET_DIR]:\n",
        "    if not os.path.exists(path):\n",
        "        logger.error(f\"Path not found: {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATPBYsIoP6DB",
        "outputId": "e70910c6-5687-437f-c02f-d756b09a465e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[38;5;39m2025-06-07 11:20:18 - INFO - Using Ultralytics YOLO version: 8.3.151\u001b[0m\n",
            "INFO:YOLOComparison:Using Ultralytics YOLO version: 8.3.151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def plot_comparison_grid(images, detections_list, class_names, method_names, save_path):\n",
        "    \"\"\"\n",
        "    images: list of np.array images [img1, img2]\n",
        "    detections_list: list of lists, shape [n_methods][n_images][detections]\n",
        "    class_names: list of str\n",
        "    method_names: list of str, e.g. ['YOLOv8', 'Enhanced YOLO', 'SAHI']\n",
        "    save_path: where to save the output image\n",
        "    \"\"\"\n",
        "    n_images = len(images)\n",
        "    n_methods = len(method_names)\n",
        "    fig, axes = plt.subplots(n_images, n_methods, figsize=(5 * n_methods, 5 * n_images))\n",
        "\n",
        "    for i in range(n_images):\n",
        "        for j in range(n_methods):\n",
        "            ax = axes[i, j] if n_images > 1 else axes[j]\n",
        "            ax.imshow(images[i])\n",
        "            # Draw detections for this method/image\n",
        "            for det in detections_list[j][i]:\n",
        "                bbox = det['bbox']\n",
        "                label = class_names[det['class_id']]\n",
        "                conf = det['confidence']\n",
        "                rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3],\n",
        "                                         linewidth=2, edgecolor='r', facecolor='none')\n",
        "                ax.add_patch(rect)\n",
        "                ax.text(bbox[0], bbox[1]-5, f'{label}: {conf:.2f}',\n",
        "                        color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
        "            if i == 0:\n",
        "                ax.set_title(method_names[j])\n",
        "            ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"[INFO] Saved comparison grid to {save_path}\")"
      ],
      "metadata": {
        "id": "YitjVznERKu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. ResultsVisualizer Class (data visualization)\n",
        "\n",
        "class ResultsVisualizer:\n",
        "    \"\"\"Handles all visualization tasks\"\"\"\n",
        "    def __init__(self, base_dir):\n",
        "        self.base_dir = base_dir\n",
        "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        self.results_dir = os.path.join(base_dir, f'comparison_results_{self.timestamp}')\n",
        "        self.dirs = {\n",
        "            'plots': os.path.join(self.results_dir, 'plots'),\n",
        "            'metrics': os.path.join(self.results_dir, 'metrics'),\n",
        "            'detections': os.path.join(self.results_dir, 'detection_examples'),\n",
        "            'logs': os.path.join(self.results_dir, 'logs')\n",
        "        }\n",
        "        for dir_path in self.dirs.values():\n",
        "            os.makedirs(dir_path, exist_ok=True)\n",
        "        logger.info(f\"Created results directory at {self.results_dir}\")\n",
        "\n",
        "    def plot_metrics_comparison(self, metrics_dict):\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        df = pd.DataFrame(metrics_dict).T\n",
        "        ax = df.plot(kind='bar', width=0.8)\n",
        "        plt.title('Performance Comparison Across Models')\n",
        "        plt.xlabel('Model Type')\n",
        "        plt.ylabel('Score')\n",
        "        for container in ax.containers:\n",
        "            ax.bar_label(container, fmt='%.3f')\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(self.dirs['plots'], 'metrics_comparison.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        csv_path = os.path.join(self.dirs['metrics'], 'metrics_comparison.csv')\n",
        "        df.to_csv(csv_path)\n",
        "        markdown_path = os.path.join(self.dirs['metrics'], 'metrics_summary.md')\n",
        "        with open(markdown_path, 'w') as f:\n",
        "            f.write(\"# Model Performance Comparison\\n\\n\")\n",
        "            f.write(df.to_markdown())\n",
        "        return df\n",
        "\n",
        "    def plot_confusion_matrix(self, true_labels, pred_labels, class_names, model_name):\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        cm = confusion_matrix(true_labels, pred_labels)\n",
        "        sns.heatmap(cm, annot=True, fmt='d',\n",
        "                    xticklabels=class_names,\n",
        "                    yticklabels=class_names)\n",
        "        plt.title(f'Confusion Matrix - {model_name}')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        save_path = os.path.join(self.dirs['plots'], f'confusion_matrix_{model_name}.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_precision_recall_curves(self, precisions, recalls, model_names):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for i, model_name in enumerate(model_names):\n",
        "            plt.plot(recalls[i], precisions[i], label=model_name)\n",
        "        plt.title('Precision-Recall Curves')\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        save_path = os.path.join(self.dirs['plots'], 'precision_recall_curves.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_per_class_map(self, class_maps, class_names, model_names):\n",
        "        df = pd.DataFrame(class_maps, index=model_names, columns=class_names)\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        ax = df.plot(kind='bar', width=0.8)\n",
        "        plt.title('Per-Class mAP Comparison')\n",
        "        plt.xlabel('Model')\n",
        "        plt.ylabel('mAP')\n",
        "        plt.legend(title='Classes', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        for container in ax.containers:\n",
        "            ax.bar_label(container, fmt='%.3f', rotation=90)\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(self.dirs['plots'], 'per_class_map.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def create_detection_grid(self, images, detections, class_names, model_names):\n",
        "        n_images = len(images)\n",
        "        n_models = len(model_names)\n",
        "        fig, axes = plt.subplots(n_images, n_models, figsize=(5*n_models, 5*n_images))\n",
        "        for i in range(n_images):\n",
        "            for j in range(n_models):\n",
        "                ax = axes[i, j] if n_images > 1 else axes[j]\n",
        "                ax.imshow(images[i])\n",
        "                for det in detections[j][i]:\n",
        "                    bbox = det['bbox']\n",
        "                    label = class_names[det['class_id']]\n",
        "                    conf = det['confidence']\n",
        "                    rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3],\n",
        "                                             linewidth=2, edgecolor='r', facecolor='none')\n",
        "                    ax.add_patch(rect)\n",
        "                    ax.text(bbox[0], bbox[1]-5, f'{label}: {conf:.2f}',\n",
        "                            color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
        "                if i == 0:\n",
        "                    ax.set_title(model_names[j])\n",
        "                ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(self.dirs['detections'], 'detection_grid.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def save_markdown_report(self, metrics_df, additional_notes=None):\n",
        "        report_path = os.path.join(self.dirs['metrics'], 'complete_report.md')\n",
        "        with open(report_path, 'w') as f:\n",
        "            f.write(\"# Model Comparison Report\\n\\n\")\n",
        "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "            f.write(\"## Summary Metrics\\n\")\n",
        "            f.write(metrics_df.to_markdown())\n",
        "            f.write(\"\\n\\n\")\n",
        "            f.write(\"## Visualization Directory Structure\\n\")\n",
        "            for dir_name, dir_path in self.dirs.items():\n",
        "                f.write(f\"- {dir_name}: {dir_path}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "            if additional_notes:\n",
        "                f.write(\"## Additional Notes\\n\")\n",
        "                f.write(additional_notes)\n",
        "                f.write(\"\\n\")\n",
        "            f.write(\"\\n## Plots Generated\\n\")\n",
        "            f.write(\"1. Metrics Comparison (Bar Plot)\\n\")\n",
        "            f.write(\"2. Confusion Matrices\\n\")\n",
        "            f.write(\"3. Precision-Recall Curves\\n\")\n",
        "            f.write(\"4. Per-Class mAP Comparison\\n\")\n",
        "            f.write(\"5. Detection Examples Grid\\n\")\n",
        "\n",
        "    def generate_all_visualizations(self, results_dict, class_names):\n",
        "        # 1. Overall metrics comparison\n",
        "        metrics_df = self.plot_metrics_comparison(results_dict['summary_metrics'])\n",
        "        # 2. Confusion matrices\n",
        "        for model_name in results_dict['confusion_matrices']:\n",
        "            cm_data = results_dict['confusion_matrices'][model_name]\n",
        "            self.plot_confusion_matrix(\n",
        "                cm_data['true'],\n",
        "                cm_data['pred'],\n",
        "                class_names,\n",
        "                model_name\n",
        "            )\n",
        "        # 3. Precision-recall curves\n",
        "        self.plot_precision_recall_curves(\n",
        "            results_dict['precision'],\n",
        "            results_dict['recall'],\n",
        "            list(results_dict['summary_metrics'].keys())\n",
        "        )\n",
        "        # 4. Per-class mAP\n",
        "        if 'per_class_map' in results_dict:\n",
        "            self.plot_per_class_map(\n",
        "                results_dict['per_class_map'],\n",
        "                class_names,\n",
        "                list(results_dict['summary_metrics'].keys())\n",
        "            )\n",
        "        # 5. Detection grid\n",
        "        if 'example_images' in results_dict and 'example_detections' in results_dict:\n",
        "            self.create_detection_grid(\n",
        "                results_dict['example_images'],\n",
        "                results_dict['example_detections'],\n",
        "                class_names,\n",
        "                list(results_dict['summary_metrics'].keys())\n",
        "            )\n",
        "        # 6. Markdown report\n",
        "        self.save_markdown_report(\n",
        "            metrics_df,\n",
        "            additional_notes=results_dict.get('notes', None)\n",
        "        )\n",
        "        logger.info(f\"All visualizations saved in {self.results_dir}\")\n",
        "        return self.results_dir"
      ],
      "metadata": {
        "id": "SJ2I9DmikLNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelEvaluator:\n",
        "    def __init__(self, base_dir, data_yaml, dataset_dir):\n",
        "        self.base_dir = base_dir\n",
        "        self.data_yaml = data_yaml\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.visualizer = ResultsVisualizer(base_dir)\n",
        "\n",
        "        # Load class names from yaml\n",
        "        with open(data_yaml, 'r') as f:\n",
        "            self.data_config = yaml.safe_load(f)\n",
        "        self.class_names = self.data_config['names']\n",
        "\n",
        "        logger.info(f\"Initialized evaluator with {len(self.class_names)} classes\")\n",
        "\n",
        "    def train_and_evaluate_baseline(self):\n",
        "        \"\"\"Train and evaluate baseline YOLOv8 model\"\"\"\n",
        "        logger.info(\"Starting baseline model training...\")\n",
        "\n",
        "        model = YOLO('yolov8n.pt')\n",
        "        results = model.train(\n",
        "            data=self.data_yaml,\n",
        "            epochs=1,\n",
        "            imgsz=640,\n",
        "            project=self.base_dir,\n",
        "            name='baseline_model'\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        val_results = model.val(data=self.data_yaml)\n",
        "        return model, val_results\n",
        "\n",
        "    def train_and_evaluate_enhanced(self):\n",
        "        \"\"\"Train and evaluate enhanced model (larger size + TTA)\"\"\"\n",
        "        logger.info(\"Starting enhanced model training...\")\n",
        "\n",
        "        model = YOLO('yolov8n.pt')\n",
        "        results = model.train(\n",
        "            data=self.data_yaml,\n",
        "            epochs=1,\n",
        "            imgsz=1024,\n",
        "            project=self.base_dir,\n",
        "            name='enhanced_model'\n",
        "        )\n",
        "\n",
        "        # Evaluate with TTA\n",
        "        val_results = model.val(\n",
        "            data=self.data_yaml,\n",
        "            imgsz=1024,\n",
        "            augment=True\n",
        "        )\n",
        "        return model, val_results\n",
        "\n",
        "    def evaluate_with_sahi(self, model_path):\n",
        "        \"\"\"Evaluate using SAHI with better error handling\"\"\"\n",
        "        try:\n",
        "            from sahi import AutoDetectionModel\n",
        "            from sahi.predict import get_sliced_prediction\n",
        "\n",
        "            logger.info(f\"Loading model from {model_path}\")\n",
        "            if not os.path.exists(model_path):\n",
        "                raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "\n",
        "            detection_model = AutoDetectionModel.from_pretrained(\n",
        "                model_type='ultralytics',\n",
        "                model_path=model_path,\n",
        "                confidence_threshold=0.3,\n",
        "                device='cuda'\n",
        "            )\n",
        "\n",
        "            test_images_dir = os.path.join(self.dataset_dir, 'test/images')\n",
        "            if not os.path.exists(test_images_dir):\n",
        "                raise FileNotFoundError(f\"Test images dir not found: {test_images_dir}\")\n",
        "\n",
        "            results = []\n",
        "            for image_name in os.listdir(test_images_dir):\n",
        "                if image_name.endswith(('.jpg', '.png')):\n",
        "                    image_path = os.path.join(test_images_dir, image_name)\n",
        "                    try:\n",
        "                        result = get_sliced_prediction(\n",
        "                            image=image_path,\n",
        "                            detection_model=detection_model,\n",
        "                            slice_height=512,\n",
        "                            slice_width=512,\n",
        "                            overlap_height_ratio=0.2,\n",
        "                            overlap_width_ratio=0.2\n",
        "                        )\n",
        "                        results.append(result)\n",
        "                        logger.debug(f\"Processed {image_name} successfully\")\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Failed to process {image_name}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"SAHI evaluation failed: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def run_complete_evaluation(self):\n",
        "        \"\"\"Run complete evaluation pipeline\"\"\"\n",
        "        try:\n",
        "            # 1. Baseline evaluation\n",
        "            baseline_model, baseline_results = self.train_and_evaluate_baseline()\n",
        "\n",
        "            # 2. Enhanced evaluation\n",
        "            enhanced_model, enhanced_results = self.train_and_evaluate_enhanced()\n",
        "\n",
        "            # 3. SAHI evaluation\n",
        "            sahi_results = self.evaluate_with_sahi(\n",
        "                f'{self.base_dir}/enhanced_model/weights/best.pt'\n",
        "            )\n",
        "\n",
        "            # Function for safe metric extraction\n",
        "            def safe_get_metrics(results):\n",
        "                \"\"\"Safe metric extraction with fallbacks\"\"\"\n",
        "                if not hasattr(results, 'box'):\n",
        "                    logger.error(\"Validation results missing 'box' attribute\")\n",
        "                    return {\n",
        "                        'mAP50': 0,\n",
        "                        'mAP50-95': 0,\n",
        "                        'recall': 0,\n",
        "                        'precision': 0\n",
        "                    }\n",
        "\n",
        "                box = results.box\n",
        "                return {\n",
        "                    'mAP50': getattr(box, 'map50', 0),\n",
        "                    'mAP50-95': getattr(box, 'map', 0),\n",
        "                    'recall': getattr(box, 'r', 0),\n",
        "                    'precision': getattr(box, 'p', 0)\n",
        "                }\n",
        "\n",
        "            # 4. Collect all results\n",
        "            baseline_metrics = safe_get_metrics(baseline_results)\n",
        "            enhanced_metrics = safe_get_metrics(enhanced_results)\n",
        "\n",
        "            results_dict = {\n",
        "                'summary_metrics': {\n",
        "                    'Baseline': {\n",
        "                        'mAP50': baseline_metrics['mAP50'],\n",
        "                        'mAP50-95': baseline_metrics['mAP50-95'],\n",
        "                        'recall': baseline_metrics['recall']\n",
        "                    },\n",
        "                    'Enhanced+TTA': {\n",
        "                        'mAP50': enhanced_metrics['mAP50'],\n",
        "                        'mAP50-95': enhanced_metrics['mAP50-95'],\n",
        "                        'recall': enhanced_metrics['recall']\n",
        "                    }\n",
        "                },\n",
        "                # Dummy confusion matrices\n",
        "                'confusion_matrices': {\n",
        "                    'Baseline': {\n",
        "                        'true': [0]*len(self.class_names),\n",
        "                        'pred': [0]*len(self.class_names)\n",
        "                    },\n",
        "                    'Enhanced+TTA': {\n",
        "                        'true': [0]*len(self.class_names),\n",
        "                        'pred': [0]*len(self.class_names)\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # 5. Generate visualizations\n",
        "            self.visualizer.generate_all_visualizations(\n",
        "                results_dict,\n",
        "                self.class_names\n",
        "            )\n",
        "\n",
        "            logger.info(\"Evaluation completed successfully!\")\n",
        "            return results_dict\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during evaluation: {str(e)}\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    def test_metrics_extraction(self):\n",
        "        \"\"\"Test metrics extraction works with current YOLO version\"\"\"\n",
        "        from unittest.mock import MagicMock\n",
        "\n",
        "        # Create mock results object\n",
        "        mock_results = MagicMock()\n",
        "        mock_results.box = MagicMock()\n",
        "        mock_results.box.map50 = 0.5\n",
        "        mock_results.box.map = 0.4\n",
        "        mock_results.box.r = 0.3\n",
        "        mock_results.box.p = 0.6\n",
        "        mock_results.confusion_matrix = None\n",
        "\n",
        "        # Since safe_get_metrics is inside run_complete_evaluation, redefining it here\n",
        "        def safe_get_metrics(results):\n",
        "            if not hasattr(results, 'box'):\n",
        "                return {\n",
        "                    'mAP50': 0,\n",
        "                    'mAP50-95': 0,\n",
        "                    'recall': 0,\n",
        "                    'precision': 0\n",
        "                }\n",
        "\n",
        "            box = results.box\n",
        "            return {\n",
        "                'mAP50': getattr(box, 'map50', 0),\n",
        "                'mAP50-95': getattr(box, 'map', 0),\n",
        "                'recall': getattr(box, 'r', 0),\n",
        "                'precision': getattr(box, 'p', 0)\n",
        "            }\n",
        "\n",
        "        metrics = safe_get_metrics(mock_results)\n",
        "        assert metrics['mAP50'] == 0.5\n",
        "        assert metrics['recall'] == 0.3\n",
        "        logger.info(\"✅ Metrics extraction test passed!\")\n",
        "\n",
        "        # Test missing attribute handling\n",
        "        mock_results.box = None\n",
        "        metrics = safe_get_metrics(mock_results)\n",
        "        assert metrics['mAP50'] == 0\n",
        "        logger.info(\"✅ Error handling test passed!\")"
      ],
      "metadata": {
        "id": "scgNqIKyQisn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelEvaluator:\n",
        "    def __init__(self, base_dir, data_yaml, dataset_dir):\n",
        "        self.base_dir = base_dir\n",
        "        self.data_yaml = data_yaml\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.visualizer = ResultsVisualizer(base_dir)\n",
        "\n",
        "        # Load class names from yaml\n",
        "        with open(data_yaml, 'r') as f:\n",
        "            self.data_config = yaml.safe_load(f)\n",
        "        self.class_names = self.data_config['names']\n",
        "\n",
        "        logger.info(f\"Initialized evaluator with {len(self.class_names)} classes\")\n",
        "\n",
        "    def train_and_evaluate_baseline(self):\n",
        "        \"\"\"Train and evaluate baseline YOLOv8 model\"\"\"\n",
        "        logger.info(\"Starting baseline model training...\")\n",
        "\n",
        "        model = YOLO('yolov8n.pt')\n",
        "        results = model.train(\n",
        "            data=self.data_yaml,\n",
        "            epochs=2,  # change to 2 for quick testing, increase as needed\n",
        "            imgsz=640,\n",
        "            project=self.base_dir,\n",
        "            name='baseline_model'\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        val_results = model.val(data=self.data_yaml)\n",
        "        return model, val_results\n",
        "\n",
        "    def train_and_evaluate_enhanced(self):\n",
        "        \"\"\"Train and evaluate enhanced model (larger size + TTA)\"\"\"\n",
        "        logger.info(\"Starting enhanced model training...\")\n",
        "\n",
        "        model = YOLO('yolov8n.pt')\n",
        "        results = model.train(\n",
        "            data=self.data_yaml,\n",
        "            epochs=2,  # change to 2 for quick testing, increase as needed\n",
        "            imgsz=1024,\n",
        "            project=self.base_dir,\n",
        "            name='enhanced_model'\n",
        "        )\n",
        "\n",
        "        # Evaluate with TTA\n",
        "        val_results = model.val(\n",
        "            data=self.data_yaml,\n",
        "            imgsz=1024,\n",
        "            augment=True\n",
        "        )\n",
        "        return model, val_results\n",
        "\n",
        "    def evaluate_with_sahi(self, model_path):\n",
        "        \"\"\"Evaluate using SAHI with better error handling\"\"\"\n",
        "        try:\n",
        "            from sahi import AutoDetectionModel\n",
        "            from sahi.predict import get_sliced_prediction\n",
        "\n",
        "            logger.info(f\"Loading model from {model_path}\")\n",
        "            if not os.path.exists(model_path):\n",
        "                raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "\n",
        "            detection_model = AutoDetectionModel.from_pretrained(\n",
        "                model_type='ultralytics',\n",
        "                model_path=model_path,\n",
        "                confidence_threshold=0.3,\n",
        "                device='cuda'\n",
        "            )\n",
        "\n",
        "            test_images_dir = os.path.join(self.dataset_dir, 'test/images')\n",
        "            if not os.path.exists(test_images_dir):\n",
        "                raise FileNotFoundError(f\"Test images dir not found: {test_images_dir}\")\n",
        "\n",
        "            results = []\n",
        "            for image_name in os.listdir(test_images_dir):\n",
        "                if image_name.endswith(('.jpg', '.png')):\n",
        "                    image_path = os.path.join(test_images_dir, image_name)\n",
        "                    try:\n",
        "                        result = get_sliced_prediction(\n",
        "                            image=image_path,\n",
        "                            detection_model=detection_model,\n",
        "                            slice_height=512,\n",
        "                            slice_width=512,\n",
        "                            overlap_height_ratio=0.2,\n",
        "                            overlap_width_ratio=0.2\n",
        "                        )\n",
        "                        results.append(result)\n",
        "                        logger.debug(f\"Processed {image_name} successfully\")\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Failed to process {image_name}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"SAHI evaluation failed: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def run_complete_evaluation(self):\n",
        "        \"\"\"Run complete evaluation pipeline\"\"\"\n",
        "        try:\n",
        "            # 1. Baseline evaluation\n",
        "            baseline_model, baseline_results = self.train_and_evaluate_baseline()\n",
        "\n",
        "            # 2. Enhanced evaluation\n",
        "            enhanced_model, enhanced_results = self.train_and_evaluate_enhanced()\n",
        "\n",
        "            # 3. SAHI evaluation\n",
        "            sahi_results = self.evaluate_with_sahi(\n",
        "                f'{self.base_dir}/enhanced_model/weights/best.pt'\n",
        "            )\n",
        "\n",
        "            # Function for safe metric extraction\n",
        "            def safe_get_metrics(results):\n",
        "                \"\"\"Safe metric extraction with fallbacks\"\"\"\n",
        "                if not hasattr(results, 'box'):\n",
        "                    logger.error(\"Validation results missing 'box' attribute\")\n",
        "                    return {\n",
        "                        'mAP50': 0,\n",
        "                        'mAP50-95': 0,\n",
        "                        'recall': 0,\n",
        "                        'precision': 0\n",
        "                    }\n",
        "\n",
        "                box = results.box\n",
        "                return {\n",
        "                    'mAP50': getattr(box, 'map50', 0),\n",
        "                    'mAP50-95': getattr(box, 'map', 0),\n",
        "                    'recall': getattr(box, 'r', 0),\n",
        "                    'precision': getattr(box, 'p', 0)\n",
        "                }\n",
        "\n",
        "            # 4. Collect all results\n",
        "            baseline_metrics = safe_get_metrics(baseline_results)\n",
        "            enhanced_metrics = safe_get_metrics(enhanced_results)\n",
        "\n",
        "            # Dummy per-class precision and recall, for each model, for plotting (set to zeros for now)\n",
        "            precision_dummy = [\n",
        "                [0]*len(self.class_names),\n",
        "                [0]*len(self.class_names)\n",
        "            ]\n",
        "            recall_dummy = [\n",
        "                [0]*len(self.class_names),\n",
        "                [0]*len(self.class_names)\n",
        "            ]\n",
        "\n",
        "            results_dict = {\n",
        "                'summary_metrics': {\n",
        "                    'Baseline': {\n",
        "                        'mAP50': baseline_metrics['mAP50'],\n",
        "                        'mAP50-95': baseline_metrics['mAP50-95'],\n",
        "                        'recall': baseline_metrics['recall']\n",
        "                    },\n",
        "                    'Enhanced+TTA': {\n",
        "                        'mAP50': enhanced_metrics['mAP50'],\n",
        "                        'mAP50-95': enhanced_metrics['mAP50-95'],\n",
        "                        'recall': enhanced_metrics['recall']\n",
        "                    }\n",
        "                },\n",
        "                # Dummy confusion matrices\n",
        "                'confusion_matrices': {\n",
        "                    'Baseline': {\n",
        "                        'true': [0]*len(self.class_names),\n",
        "                        'pred': [0]*len(self.class_names)\n",
        "                    },\n",
        "                    'Enhanced+TTA': {\n",
        "                        'true': [0]*len(self.class_names),\n",
        "                        'pred': [0]*len(self.class_names)\n",
        "                    }\n",
        "                },\n",
        "                # Add dummy precision and recall for plotting\n",
        "                'precision': precision_dummy,\n",
        "                'recall': recall_dummy\n",
        "            }\n",
        "\n",
        "            # 5. Generate visualizations\n",
        "            self.visualizer.generate_all_visualizations(\n",
        "                results_dict,\n",
        "                self.class_names\n",
        "            )\n",
        "\n",
        "            logger.info(\"Evaluation completed successfully!\")\n",
        "            return results_dict\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during evaluation: {str(e)}\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    def test_metrics_extraction(self):\n",
        "        \"\"\"Test metrics extraction works with current YOLO version\"\"\"\n",
        "        from unittest.mock import MagicMock\n",
        "\n",
        "        # Create mock results object\n",
        "        mock_results = MagicMock()\n",
        "        mock_results.box = MagicMock()\n",
        "        mock_results.box.map50 = 0.5\n",
        "        mock_results.box.map = 0.4\n",
        "        mock_results.box.r = 0.3\n",
        "        mock_results.box.p = 0.6\n",
        "        mock_results.confusion_matrix = None\n",
        "\n",
        "        # Since safe_get_metrics is inside run_complete_evaluation, redefining it here\n",
        "        def safe_get_metrics(results):\n",
        "            if not hasattr(results, 'box'):\n",
        "                return {\n",
        "                    'mAP50': 0,\n",
        "                    'mAP50-95': 0,\n",
        "                    'recall': 0,\n",
        "                    'precision': 0\n",
        "                }\n",
        "\n",
        "            box = results.box\n",
        "            return {\n",
        "                'mAP50': getattr(box, 'map50', 0),\n",
        "                'mAP50-95': getattr(box, 'map', 0),\n",
        "                'recall': getattr(box, 'r', 0),\n",
        "                'precision': getattr(box, 'p', 0)\n",
        "            }\n",
        "\n",
        "        metrics = safe_get_metrics(mock_results)\n",
        "        assert metrics['mAP50'] == 0.5\n",
        "        assert metrics['recall'] == 0.3\n",
        "        logger.info(\"✅ Metrics extraction test passed!\")\n",
        "\n",
        "        # Test missing attribute handling\n",
        "        mock_results.box = None\n",
        "        metrics = safe_get_metrics(mock_results)\n",
        "        assert metrics['mAP50'] == 0\n",
        "        logger.info(\"✅ Error handling test passed!\")"
      ],
      "metadata": {
        "id": "SbtaCT3WQoA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Main Pipeline\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = ModelEvaluator(BASE_DIR, DATA_YAML, DATASET_DIR)\n",
        "evaluator.test_metrics_extraction()\n",
        "\n",
        "# Run evaluation\n",
        "results = evaluator.run_complete_evaluation()\n",
        "\n",
        "logger.info(f\"Results saved in: {evaluator.visualizer.results_dir}\")"
      ],
      "metadata": {
        "id": "Vd6Z6HEmQtUY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cb19bd52-1897-48f3-d90d-86fd62de943c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[38;5;39m2025-06-07 11:21:02 - INFO - Created results directory at /content/drive/MyDrive/new scope model/comparison_results_20250607_112102\u001b[0m\n",
            "INFO:YOLOComparison:Created results directory at /content/drive/MyDrive/new scope model/comparison_results_20250607_112102\n",
            "\u001b[38;5;39m2025-06-07 11:21:02 - INFO - Initialized evaluator with 8 classes\u001b[0m\n",
            "INFO:YOLOComparison:Initialized evaluator with 8 classes\n",
            "\u001b[38;5;39m2025-06-07 11:21:02 - INFO - ✅ Metrics extraction test passed!\u001b[0m\n",
            "INFO:YOLOComparison:✅ Metrics extraction test passed!\n",
            "\u001b[38;5;39m2025-06-07 11:21:02 - INFO - ✅ Error handling test passed!\u001b[0m\n",
            "INFO:YOLOComparison:✅ Error handling test passed!\n",
            "\u001b[38;5;39m2025-06-07 11:21:02 - INFO - Starting baseline model training...\u001b[0m\n",
            "INFO:YOLOComparison:Starting baseline model training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.151 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/new scope model/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=2, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=baseline_model12, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/new scope model, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/new scope model/baseline_model12, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.2±0.1 ms, read: 345.5±68.6 MB/s, size: 791.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Samplesmall_dataset/train/labels.cache... 800 images, 0 backgrounds, 0 corrupt: 100%|██████████| 800/800 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mSlow image access detected (ping: 21.2±45.5 ms, read: 79.3±78.8 MB/s, size: 800.0 KB). Use local storage instead of remote/mounted storage for better performance. See https://docs.ultralytics.com/guides/model-training-tips/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Samplesmall_dataset/val/labels.cache... 200 images, 0 backgrounds, 0 corrupt: 100%|██████████| 200/200 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to /content/drive/MyDrive/new scope model/baseline_model12/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/new scope model/baseline_model12\u001b[0m\n",
            "Starting training for 2 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/2      2.24G      1.859      2.989      1.262        156        640: 100%|██████████| 50/50 [00:05<00:00,  8.80it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  7.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200       1235     0.0148      0.365      0.106      0.058\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/2      2.26G      1.662      1.823      1.186        205        640: 100%|██████████| 50/50 [00:04<00:00, 10.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:00<00:00,  7.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200       1235       0.82      0.118      0.243      0.133\n",
            "\n",
            "2 epochs completed in 0.004 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/new scope model/baseline_model12/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/drive/MyDrive/new scope model/baseline_model12/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/drive/MyDrive/new scope model/baseline_model12/weights/best.pt...\n",
            "Ultralytics 8.3.151 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:01<00:00,  4.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200       1235      0.821      0.118      0.242      0.134\n",
            "                   Car        104        491      0.923      0.387      0.688      0.424\n",
            "            Pedestrian        200        530      0.842      0.303       0.54      0.236\n",
            "                   Van         60         75      0.719       0.16      0.229      0.153\n",
            "               Cyclist         53         90       0.66     0.0218      0.166       0.07\n",
            "        Person_sitting          7         19          1          0      0.115     0.0655\n",
            "                  Misc          6          6          1          0    0.00147   0.000441\n",
            "                 Truck         13         14      0.422     0.0714      0.177      0.115\n",
            "                  Tram          3         10          1          0     0.0217     0.0101\n",
            "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/new scope model/baseline_model12\u001b[0m\n",
            "Ultralytics 8.3.151 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.4±0.5 ms, read: 307.0±178.0 MB/s, size: 821.8 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Samplesmall_dataset/val/labels.cache... 200 images, 0 backgrounds, 0 corrupt: 100%|██████████| 200/200 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:01<00:00,  6.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200       1235      0.819      0.118      0.242      0.134\n",
            "                   Car        104        491      0.926      0.389      0.688      0.424\n",
            "            Pedestrian        200        530      0.833      0.298      0.538      0.237\n",
            "                   Van         60         75      0.718       0.16      0.229      0.153\n",
            "               Cyclist         53         90      0.659     0.0217      0.166     0.0715\n",
            "        Person_sitting          7         19          1          0      0.115     0.0654\n",
            "                  Misc          6          6          1          0    0.00148   0.000482\n",
            "                 Truck         13         14      0.413     0.0714      0.177      0.115\n",
            "                  Tram          3         10          1          0     0.0217     0.0101\n",
            "Speed: 0.2ms preprocess, 0.7ms inference, 0.0ms loss, 3.6ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/new scope model/baseline_model122\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[38;5;39m2025-06-07 11:21:35 - INFO - Starting enhanced model training...\u001b[0m\n",
            "INFO:YOLOComparison:Starting enhanced model training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.151 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/new scope model/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=2, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1024, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=enhanced_model6, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/new scope model, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/new scope model/enhanced_model6, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.3±0.1 ms, read: 312.4±48.8 MB/s, size: 791.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Samplesmall_dataset/train/labels.cache... 800 images, 0 backgrounds, 0 corrupt: 100%|██████████| 800/800 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.5±0.2 ms, read: 219.9±113.2 MB/s, size: 800.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Samplesmall_dataset/val/labels.cache... 200 images, 0 backgrounds, 0 corrupt: 100%|██████████| 200/200 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to /content/drive/MyDrive/new scope model/enhanced_model6/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/new scope model/enhanced_model6\u001b[0m\n",
            "Starting training for 2 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/2      5.57G      1.763       3.09      1.313        158       1024: 100%|██████████| 50/50 [00:07<00:00,  6.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:01<00:00,  6.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200       1235      0.021      0.518      0.134     0.0727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/2      5.59G      1.534      2.023      1.222        205       1024: 100%|██████████| 50/50 [00:07<00:00,  6.70it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:01<00:00,  5.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200       1235      0.871      0.137      0.283      0.166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2 epochs completed in 0.005 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/new scope model/enhanced_model6/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/drive/MyDrive/new scope model/enhanced_model6/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/drive/MyDrive/new scope model/enhanced_model6/weights/best.pt...\n",
            "Ultralytics 8.3.151 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:01<00:00,  4.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200       1235       0.87      0.137      0.283      0.166\n",
            "                   Car        104        491      0.913        0.3      0.713       0.46\n",
            "            Pedestrian        200        530      0.955      0.198      0.562      0.257\n",
            "                   Van         60         75      0.603      0.182       0.26      0.197\n",
            "               Cyclist         53         90      0.816      0.133      0.398      0.185\n",
            "        Person_sitting          7         19          1          0     0.0137    0.00624\n",
            "                  Misc          6          6          1          0   0.000116   1.16e-05\n",
            "                 Truck         13         14      0.673      0.286      0.289      0.209\n",
            "                  Tram          3         10          1          0     0.0257     0.0104\n",
            "Speed: 0.1ms preprocess, 1.4ms inference, 0.0ms loss, 3.4ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/new scope model/enhanced_model6\u001b[0m\n",
            "Ultralytics 8.3.151 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.3±0.1 ms, read: 377.2±37.8 MB/s, size: 821.8 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Samplesmall_dataset/val/labels.cache... 200 images, 0 backgrounds, 0 corrupt: 100%|██████████| 200/200 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:03<00:00,  4.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200       1235      0.846      0.123      0.257      0.146\n",
            "                   Car        104        491      0.934       0.32      0.694      0.445\n",
            "            Pedestrian        200        530      0.956      0.123      0.541      0.252\n",
            "                   Van         60         75      0.451       0.11      0.194      0.135\n",
            "               Cyclist         53         90      0.871       0.15      0.287      0.132\n",
            "        Person_sitting          7         19          1          0     0.0364     0.0179\n",
            "                  Misc          6          6          1          0   0.000559   5.59e-05\n",
            "                 Truck         13         14      0.552      0.286      0.243      0.158\n",
            "                  Tram          3         10          1          0     0.0575      0.027\n",
            "Speed: 1.1ms preprocess, 3.5ms inference, 0.0ms loss, 3.6ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/new scope model/enhanced_model62\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[38;5;39m2025-06-07 11:22:12 - INFO - Loading model from /content/drive/MyDrive/new scope model/enhanced_model/weights/best.pt\u001b[0m\n",
            "INFO:YOLOComparison:Loading model from /content/drive/MyDrive/new scope model/enhanced_model/weights/best.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "\u001b[38;5;39m2025-06-07 11:22:50 - INFO - All visualizations saved in /content/drive/MyDrive/new scope model/comparison_results_20250607_112102\u001b[0m\n",
            "INFO:YOLOComparison:All visualizations saved in /content/drive/MyDrive/new scope model/comparison_results_20250607_112102\n",
            "\u001b[38;5;39m2025-06-07 11:22:50 - INFO - Evaluation completed successfully!\u001b[0m\n",
            "INFO:YOLOComparison:Evaluation completed successfully!\n",
            "\u001b[38;5;39m2025-06-07 11:22:50 - INFO - Results saved in: /content/drive/MyDrive/new scope model/comparison_results_20250607_112102\u001b[0m\n",
            "INFO:YOLOComparison:Results saved in: /content/drive/MyDrive/new scope model/comparison_results_20250607_112102\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}