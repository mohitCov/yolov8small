{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQmIR21EPmTc",
        "outputId": "65c3bcb7-60a7-4e52-d844-4818df4bdbee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[38;5;39m2025-06-07 09:56:12 - INFO - Logger initialized\u001b[0m\n",
            "INFO:YOLOComparison:Logger initialized\n"
          ]
        }
      ],
      "source": [
        "# 1. Logging Setup (run first)\n",
        "\n",
        "import logging\n",
        "\n",
        "class CustomFormatter(logging.Formatter):\n",
        "    \"\"\"Custom formatter with colors for notebook output\"\"\"\n",
        "    grey = \"\\x1b[38;21m\"\n",
        "    blue = \"\\x1b[38;5;39m\"\n",
        "    yellow = \"\\x1b[38;5;226m\"\n",
        "    red = \"\\x1b[38;5;196m\"\n",
        "    reset = \"\\x1b[0m\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "                         datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
        "        self.FORMATS = {\n",
        "            logging.DEBUG: self.grey,\n",
        "            logging.INFO: self.blue,\n",
        "            logging.WARNING: self.yellow,\n",
        "            logging.ERROR: self.red\n",
        "        }\n",
        "\n",
        "    def format(self, record):\n",
        "        color = self.FORMATS.get(record.levelno, self.grey)\n",
        "        log_fmt = f\"{color}%(asctime)s - %(levelname)s - %(message)s{self.reset}\"\n",
        "        formatter = logging.Formatter(log_fmt, datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
        "        return formatter.format(record)\n",
        "\n",
        "# Setup logger\n",
        "logger = logging.getLogger(\"YOLOComparison\")\n",
        "logger.setLevel(logging.INFO)\n",
        "if not logger.handlers:\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setFormatter(CustomFormatter())\n",
        "    logger.addHandler(console_handler)\n",
        "logger.info(\"Logger initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all required packages (run this first if you get ModuleNotFoundError)\n",
        "!pip install ultralytics sahi pycocotools pandas seaborn --quiet"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ma-uyddPQD93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d1d263d-1c12-47c9-f608-2f172f4db52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Imports & Config (run after logging setup)\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import matplotlib.patches as patches\n",
        "from ultralytics import YOLO, __version__ as yolo_version\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Google Drive mounting (run only in Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path configs (Change if needed)\n",
        "BASE_DIR = '/content/drive/MyDrive/new scope model'\n",
        "DATA_YAML = f'{BASE_DIR}/data.yaml'\n",
        "DATASET_DIR = '/content/drive/MyDrive/Samplesmall_dataset'\n",
        "\n",
        "logger.info(f\"Using Ultralytics YOLO version: {yolo_version}\")\n",
        "for path in [BASE_DIR, DATA_YAML, DATASET_DIR]:\n",
        "    if not os.path.exists(path):\n",
        "        logger.error(f\"Path not found: {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATPBYsIoP6DB",
        "outputId": "18fc1356-289f-4620-98fe-029b275f869e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[38;5;39m2025-06-07 09:58:12 - INFO - Using Ultralytics YOLO version: 8.3.151\u001b[0m\n",
            "INFO:YOLOComparison:Using Ultralytics YOLO version: 8.3.151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def plot_comparison_grid(images, detections_list, class_names, method_names, save_path):\n",
        "    \"\"\"\n",
        "    images: list of np.array images [img1, img2]\n",
        "    detections_list: list of lists, shape [n_methods][n_images][detections]\n",
        "    class_names: list of str\n",
        "    method_names: list of str, e.g. ['YOLOv8', 'Enhanced YOLO', 'SAHI']\n",
        "    save_path: where to save the output image\n",
        "    \"\"\"\n",
        "    n_images = len(images)\n",
        "    n_methods = len(method_names)\n",
        "    fig, axes = plt.subplots(n_images, n_methods, figsize=(5 * n_methods, 5 * n_images))\n",
        "\n",
        "    for i in range(n_images):\n",
        "        for j in range(n_methods):\n",
        "            ax = axes[i, j] if n_images > 1 else axes[j]\n",
        "            ax.imshow(images[i])\n",
        "            # Draw detections for this method/image\n",
        "            for det in detections_list[j][i]:\n",
        "                bbox = det['bbox']\n",
        "                label = class_names[det['class_id']]\n",
        "                conf = det['confidence']\n",
        "                rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3],\n",
        "                                         linewidth=2, edgecolor='r', facecolor='none')\n",
        "                ax.add_patch(rect)\n",
        "                ax.text(bbox[0], bbox[1]-5, f'{label}: {conf:.2f}',\n",
        "                        color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
        "            if i == 0:\n",
        "                ax.set_title(method_names[j])\n",
        "            ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"[INFO] Saved comparison grid to {save_path}\")"
      ],
      "metadata": {
        "id": "YitjVznERKu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. ResultsVisualizer Class (data visualization)\n",
        "\n",
        "class ResultsVisualizer:\n",
        "    \"\"\"Handles all visualization tasks\"\"\"\n",
        "    def __init__(self, base_dir):\n",
        "        self.base_dir = base_dir\n",
        "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        self.results_dir = os.path.join(base_dir, f'comparison_results_{self.timestamp}')\n",
        "        self.dirs = {\n",
        "            'plots': os.path.join(self.results_dir, 'plots'),\n",
        "            'metrics': os.path.join(self.results_dir, 'metrics'),\n",
        "            'detections': os.path.join(self.results_dir, 'detection_examples'),\n",
        "            'logs': os.path.join(self.results_dir, 'logs')\n",
        "        }\n",
        "        for dir_path in self.dirs.values():\n",
        "            os.makedirs(dir_path, exist_ok=True)\n",
        "        logger.info(f\"Created results directory at {self.results_dir}\")\n",
        "\n",
        "    def plot_metrics_comparison(self, metrics_dict):\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        df = pd.DataFrame(metrics_dict).T\n",
        "        ax = df.plot(kind='bar', width=0.8)\n",
        "        plt.title('Performance Comparison Across Models')\n",
        "        plt.xlabel('Model Type')\n",
        "        plt.ylabel('Score')\n",
        "        for container in ax.containers:\n",
        "            ax.bar_label(container, fmt='%.3f')\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(self.dirs['plots'], 'metrics_comparison.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        csv_path = os.path.join(self.dirs['metrics'], 'metrics_comparison.csv')\n",
        "        df.to_csv(csv_path)\n",
        "        markdown_path = os.path.join(self.dirs['metrics'], 'metrics_summary.md')\n",
        "        with open(markdown_path, 'w') as f:\n",
        "            f.write(\"# Model Performance Comparison\\n\\n\")\n",
        "            f.write(df.to_markdown())\n",
        "        return df\n",
        "\n",
        "    def plot_confusion_matrix(self, true_labels, pred_labels, class_names, model_name):\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        cm = confusion_matrix(true_labels, pred_labels)\n",
        "        sns.heatmap(cm, annot=True, fmt='d',\n",
        "                    xticklabels=class_names,\n",
        "                    yticklabels=class_names)\n",
        "        plt.title(f'Confusion Matrix - {model_name}')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        save_path = os.path.join(self.dirs['plots'], f'confusion_matrix_{model_name}.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_precision_recall_curves(self, precisions, recalls, model_names):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for i, model_name in enumerate(model_names):\n",
        "            plt.plot(recalls[i], precisions[i], label=model_name)\n",
        "        plt.title('Precision-Recall Curves')\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        save_path = os.path.join(self.dirs['plots'], 'precision_recall_curves.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_per_class_map(self, class_maps, class_names, model_names):\n",
        "        df = pd.DataFrame(class_maps, index=model_names, columns=class_names)\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        ax = df.plot(kind='bar', width=0.8)\n",
        "        plt.title('Per-Class mAP Comparison')\n",
        "        plt.xlabel('Model')\n",
        "        plt.ylabel('mAP')\n",
        "        plt.legend(title='Classes', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        for container in ax.containers:\n",
        "            ax.bar_label(container, fmt='%.3f', rotation=90)\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(self.dirs['plots'], 'per_class_map.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def create_detection_grid(self, images, detections, class_names, model_names):\n",
        "        n_images = len(images)\n",
        "        n_models = len(model_names)\n",
        "        fig, axes = plt.subplots(n_images, n_models, figsize=(5*n_models, 5*n_images))\n",
        "        for i in range(n_images):\n",
        "            for j in range(n_models):\n",
        "                ax = axes[i, j] if n_images > 1 else axes[j]\n",
        "                ax.imshow(images[i])\n",
        "                for det in detections[j][i]:\n",
        "                    bbox = det['bbox']\n",
        "                    label = class_names[det['class_id']]\n",
        "                    conf = det['confidence']\n",
        "                    rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3],\n",
        "                                             linewidth=2, edgecolor='r', facecolor='none')\n",
        "                    ax.add_patch(rect)\n",
        "                    ax.text(bbox[0], bbox[1]-5, f'{label}: {conf:.2f}',\n",
        "                            color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
        "                if i == 0:\n",
        "                    ax.set_title(model_names[j])\n",
        "                ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(self.dirs['detections'], 'detection_grid.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def save_markdown_report(self, metrics_df, additional_notes=None):\n",
        "        report_path = os.path.join(self.dirs['metrics'], 'complete_report.md')\n",
        "        with open(report_path, 'w') as f:\n",
        "            f.write(\"# Model Comparison Report\\n\\n\")\n",
        "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "            f.write(\"## Summary Metrics\\n\")\n",
        "            f.write(metrics_df.to_markdown())\n",
        "            f.write(\"\\n\\n\")\n",
        "            f.write(\"## Visualization Directory Structure\\n\")\n",
        "            for dir_name, dir_path in self.dirs.items():\n",
        "                f.write(f\"- {dir_name}: {dir_path}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "            if additional_notes:\n",
        "                f.write(\"## Additional Notes\\n\")\n",
        "                f.write(additional_notes)\n",
        "                f.write(\"\\n\")\n",
        "            f.write(\"\\n## Plots Generated\\n\")\n",
        "            f.write(\"1. Metrics Comparison (Bar Plot)\\n\")\n",
        "            f.write(\"2. Confusion Matrices\\n\")\n",
        "            f.write(\"3. Precision-Recall Curves\\n\")\n",
        "            f.write(\"4. Per-Class mAP Comparison\\n\")\n",
        "            f.write(\"5. Detection Examples Grid\\n\")\n",
        "\n",
        "    def generate_all_visualizations(self, results_dict, class_names):\n",
        "        # 1. Overall metrics comparison\n",
        "        metrics_df = self.plot_metrics_comparison(results_dict['summary_metrics'])\n",
        "        # 2. Confusion matrices\n",
        "        for model_name in results_dict['confusion_matrices']:\n",
        "            cm_data = results_dict['confusion_matrices'][model_name]\n",
        "            self.plot_confusion_matrix(\n",
        "                cm_data['true'],\n",
        "                cm_data['pred'],\n",
        "                class_names,\n",
        "                model_name\n",
        "            )\n",
        "        # 3. Precision-recall curves\n",
        "        self.plot_precision_recall_curves(\n",
        "            results_dict['precision'],\n",
        "            results_dict['recall'],\n",
        "            list(results_dict['summary_metrics'].keys())\n",
        "        )\n",
        "        # 4. Per-class mAP\n",
        "        if 'per_class_map' in results_dict:\n",
        "            self.plot_per_class_map(\n",
        "                results_dict['per_class_map'],\n",
        "                class_names,\n",
        "                list(results_dict['summary_metrics'].keys())\n",
        "            )\n",
        "        # 5. Detection grid\n",
        "        if 'example_images' in results_dict and 'example_detections' in results_dict:\n",
        "            self.create_detection_grid(\n",
        "                results_dict['example_images'],\n",
        "                results_dict['example_detections'],\n",
        "                class_names,\n",
        "                list(results_dict['summary_metrics'].keys())\n",
        "            )\n",
        "        # 6. Markdown report\n",
        "        self.save_markdown_report(\n",
        "            metrics_df,\n",
        "            additional_notes=results_dict.get('notes', None)\n",
        "        )\n",
        "        logger.info(f\"All visualizations saved in {self.results_dir}\")\n",
        "        return self.results_dir"
      ],
      "metadata": {
        "id": "SJ2I9DmikLNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelEvaluator:\n",
        "    def __init__(self, base_dir, data_yaml):\n",
        "        self.base_dir = base_dir\n",
        "        self.data_yaml = data_yaml\n",
        "        self.visualizer = ResultsVisualizer(base_dir)\n",
        "\n",
        "        # Load class names from yaml\n",
        "        with open(data_yaml, 'r') as f:\n",
        "            self.data_config = yaml.safe_load(f)\n",
        "        self.class_names = self.data_config['names']\n",
        "\n",
        "        logger.info(f\"Initialized evaluator with {len(self.class_names)} classes\")\n",
        "\n",
        "    def train_and_evaluate_baseline(self):\n",
        "        \"\"\"Train and evaluate baseline YOLOv8 model\"\"\"\n",
        "        logger.info(\"Starting baseline model training...\")\n",
        "\n",
        "        model = YOLO('yolov8n.pt')\n",
        "        results = model.train(\n",
        "            data=self.data_yaml,\n",
        "            epochs=100,\n",
        "            imgsz=640,\n",
        "            project=self.base_dir,\n",
        "            name='baseline_model'\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        val_results = model.val(data=self.data_yaml)\n",
        "        return model, val_results\n",
        "\n",
        "    def train_and_evaluate_enhanced(self):\n",
        "        \"\"\"Train and evaluate enhanced model (larger size + TTA)\"\"\"\n",
        "        logger.info(\"Starting enhanced model training...\")\n",
        "\n",
        "        model = YOLO('yolov8n.pt')\n",
        "        results = model.train(\n",
        "            data=self.data_yaml,\n",
        "            epochs=100,\n",
        "            imgsz=1024,\n",
        "            project=self.base_dir,\n",
        "            name='enhanced_model'\n",
        "        )\n",
        "\n",
        "        # Evaluate with TTA\n",
        "        val_results = model.val(\n",
        "            data=self.data_yaml,\n",
        "            imgsz=1024,\n",
        "            augment=True\n",
        "        )\n",
        "        return model, val_results\n",
        "\n",
        "    def evaluate_with_sahi(self, model_path):\n",
        "        \"\"\"Evaluate using SAHI with better error handling\"\"\"\n",
        "        try:\n",
        "            from sahi import AutoDetectionModel\n",
        "            from sahi.predict import get_sliced_prediction\n",
        "\n",
        "            logger.info(f\"Loading model from {model_path}\")\n",
        "            if not os.path.exists(model_path):\n",
        "                raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "\n",
        "            detection_model = AutoDetectionModel.from_pretrained(\n",
        "                model_type='ultralytics',\n",
        "                model_path=model_path,\n",
        "                confidence_threshold=0.3,\n",
        "                device='cuda'\n",
        "            )\n",
        "\n",
        "            test_images_dir = os.path.join(DATASET_DIR, 'test/images')\n",
        "            if not os.path.exists(test_images_dir):\n",
        "                raise FileNotFoundError(f\"Test images dir not found: {test_images_dir}\")\n",
        "\n",
        "            results = []\n",
        "            for image_name in os.listdir(test_images_dir):\n",
        "                if image_name.endswith(('.jpg', '.png')):\n",
        "                    image_path = os.path.join(test_images_dir, image_name)\n",
        "                    try:\n",
        "                        result = get_sliced_prediction(\n",
        "                            image=image_path,\n",
        "                            detection_model=detection_model,\n",
        "                            slice_height=512,\n",
        "                            slice_width=512,\n",
        "                            overlap_height_ratio=0.2,\n",
        "                            overlap_width_ratio=0.2\n",
        "                        )\n",
        "                        results.append(result)\n",
        "                        logger.debug(f\"Processed {image_name} successfully\")\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Failed to process {image_name}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"SAHI evaluation failed: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def run_complete_evaluation(self):\n",
        "        \"\"\"Run complete evaluation pipeline\"\"\"\n",
        "        try:\n",
        "            # 1. Baseline evaluation\n",
        "            baseline_model, baseline_results = self.train_and_evaluate_baseline()\n",
        "\n",
        "            # 2. Enhanced evaluation\n",
        "            enhanced_model, enhanced_results = self.train_and_evaluate_enhanced()\n",
        "\n",
        "            # 3. SAHI evaluation\n",
        "            sahi_results = self.evaluate_with_sahi(\n",
        "                f'{self.base_dir}/enhanced_model/weights/best.pt'\n",
        "            )\n",
        "\n",
        "            # Function for safe metric extraction\n",
        "            def safe_get_metrics(results):\n",
        "                \"\"\"Safe metric extraction with fallbacks\"\"\"\n",
        "                if not hasattr(results, 'box'):\n",
        "                    logger.error(\"Validation results missing 'box' attribute\")\n",
        "                    return {\n",
        "                        'mAP50': 0,\n",
        "                        'mAP50-95': 0,\n",
        "                        'recall': 0,\n",
        "                        'precision': 0\n",
        "                    }\n",
        "\n",
        "                box = results.box\n",
        "                return {\n",
        "                    'mAP50': getattr(box, 'map50', 0),\n",
        "                    'mAP50-95': getattr(box, 'map', 0),\n",
        "                    'recall': getattr(box, 'r', 0),\n",
        "                    'precision': getattr(box, 'p', 0)\n",
        "                }\n",
        "\n",
        "            # 4. Collect all results\n",
        "            baseline_metrics = safe_get_metrics(baseline_results)\n",
        "            enhanced_metrics = safe_get_metrics(enhanced_results)\n",
        "\n",
        "            results_dict = {\n",
        "                'summary_metrics': {\n",
        "                    'Baseline': {\n",
        "                        'mAP50': baseline_metrics['mAP50'],\n",
        "                        'mAP50-95': baseline_metrics['mAP50-95'],\n",
        "                        'recall': baseline_metrics['recall']\n",
        "                    },\n",
        "                    'Enhanced+TTA': {\n",
        "                        'mAP50': enhanced_metrics['mAP50'],\n",
        "                        'mAP50-95': enhanced_metrics['mAP50-95'],\n",
        "                        'recall': enhanced_metrics['recall']\n",
        "                    }\n",
        "                },\n",
        "                'confusion_matrices': {\n",
        "    'Baseline': {\n",
        "        'true': [0]*len(self.class_names),\n",
        "        'pred': [0]*len(self.class_names)\n",
        "    },\n",
        "    'Enhanced+TTA': {\n",
        "        'true': [0]*len(self.class_names),\n",
        "        'pred': [0]*len(self.class_names)\n",
        "    }\n",
        "}\n",
        "            }\n",
        "\n",
        "            # 5. Generate visualizations\n",
        "            self.visualizer.generate_all_visualizations(\n",
        "                results_dict,\n",
        "                self.class_names\n",
        "            )\n",
        "\n",
        "            logger.info(\"Evaluation completed successfully!\")\n",
        "            return results_dict\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during evaluation: {str(e)}\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    def test_metrics_extraction(self):\n",
        "        \"\"\"Test metrics extraction works with current YOLO version\"\"\"\n",
        "        from unittest.mock import MagicMock\n",
        "\n",
        "        # Create mock results object\n",
        "        mock_results = MagicMock()\n",
        "        mock_results.box = MagicMock()\n",
        "        mock_results.box.map50 = 0.5\n",
        "        mock_results.box.map = 0.4\n",
        "        mock_results.box.r = 0.3\n",
        "        mock_results.box.p = 0.6\n",
        "        mock_results.confusion_matrix = None\n",
        "\n",
        "        # Since safe_get_metrics is inside run_complete_evaluation, redefining it here\n",
        "        def safe_get_metrics(results):\n",
        "            if not hasattr(results, 'box'):\n",
        "                return {\n",
        "                    'mAP50': 0,\n",
        "                    'mAP50-95': 0,\n",
        "                    'recall': 0,\n",
        "                    'precision': 0\n",
        "                }\n",
        "\n",
        "            box = results.box\n",
        "            return {\n",
        "                'mAP50': getattr(box, 'map50', 0),\n",
        "                'mAP50-95': getattr(box, 'map', 0),\n",
        "                'recall': getattr(box, 'r', 0),\n",
        "                'precision': getattr(box, 'p', 0)\n",
        "            }\n",
        "\n",
        "        metrics = safe_get_metrics(mock_results)\n",
        "        assert metrics['mAP50'] == 0.5\n",
        "        assert metrics['recall'] == 0.3\n",
        "        logger.info(\"✅ Metrics extraction test passed!\")\n",
        "\n",
        "        # Test missing attribute handling\n",
        "        mock_results.box = None\n",
        "        metrics = safe_get_metrics(mock_results)\n",
        "        assert metrics['mAP50'] == 0\n",
        "        logger.info(\"✅ Error handling test passed!\")"
      ],
      "metadata": {
        "id": "scgNqIKyQisn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelEvaluator:\n",
        "    def __init__(self, base_dir, data_yaml):\n",
        "        self.base_dir = base_dir\n",
        "        self.data_yaml = data_yaml\n",
        "        self.visualizer = ResultsVisualizer(base_dir)\n",
        "\n",
        "        # Load class names from yaml\n",
        "        with open(data_yaml, 'r') as f:\n",
        "            self.data_config = yaml.safe_load(f)\n",
        "        self.class_names = self.data_config['names']\n",
        "\n",
        "        logger.info(f\"Initialized evaluator with {len(self.class_names)} classes\")\n",
        "\n",
        "    def train_and_evaluate_baseline(self):\n",
        "        \"\"\"Train and evaluate baseline YOLOv8 model\"\"\"\n",
        "        logger.info(\"Starting baseline model training...\")\n",
        "\n",
        "        model = YOLO('yolov8n.pt')\n",
        "        results = model.train(\n",
        "            data=self.data_yaml,\n",
        "            epochs=15,\n",
        "            imgsz=640,\n",
        "            project=self.base_dir,\n",
        "            name='baseline_model'\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        val_results = model.val(data=self.data_yaml)\n",
        "        return model, val_results\n",
        "\n",
        "    def train_and_evaluate_enhanced(self):\n",
        "        \"\"\"Train and evaluate enhanced model (larger size + TTA)\"\"\"\n",
        "        logger.info(\"Starting enhanced model training...\")\n",
        "\n",
        "        model = YOLO('yolov8n.pt')\n",
        "        results = model.train(\n",
        "            data=self.data_yaml,\n",
        "            epochs=15,\n",
        "            imgsz=1024,\n",
        "            project=self.base_dir,\n",
        "            name='enhanced_model'\n",
        "        )\n",
        "\n",
        "        # Evaluate with TTA\n",
        "        val_results = model.val(\n",
        "            data=self.data_yaml,\n",
        "            imgsz=1024,\n",
        "            augment=True\n",
        "        )\n",
        "        return model, val_results\n",
        "\n",
        "    def evaluate_with_sahi(self, model_path):\n",
        "        \"\"\"Evaluate using SAHI with better error handling\"\"\"\n",
        "        try:\n",
        "            from sahi import AutoDetectionModel\n",
        "            from sahi.predict import get_sliced_prediction\n",
        "\n",
        "            logger.info(f\"Loading model from {model_path}\")\n",
        "            if not os.path.exists(model_path):\n",
        "                raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "\n",
        "            detection_model = AutoDetectionModel.from_pretrained(\n",
        "                model_type='ultralytics',\n",
        "                model_path=model_path,\n",
        "                confidence_threshold=0.3,\n",
        "                device='cuda'\n",
        "            )\n",
        "\n",
        "            test_images_dir = os.path.join(DATASET_DIR, 'test/images')\n",
        "            if not os.path.exists(test_images_dir):\n",
        "                raise FileNotFoundError(f\"Test images dir not found: {test_images_dir}\")\n",
        "\n",
        "            results = []\n",
        "            for image_name in os.listdir(test_images_dir):\n",
        "                if image_name.endswith(('.jpg', '.png')):\n",
        "                    image_path = os.path.join(test_images_dir, image_name)\n",
        "                    try:\n",
        "                        result = get_sliced_prediction(\n",
        "                            image=image_path,\n",
        "                            detection_model=detection_model,\n",
        "                            slice_height=512,\n",
        "                            slice_width=512,\n",
        "                            overlap_height_ratio=0.2,\n",
        "                            overlap_width_ratio=0.2\n",
        "                        )\n",
        "                        results.append(result)\n",
        "                        logger.debug(f\"Processed {image_name} successfully\")\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Failed to process {image_name}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"SAHI evaluation failed: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def run_complete_evaluation(self):\n",
        "        \"\"\"Run complete evaluation pipeline\"\"\"\n",
        "        try:\n",
        "            # 1. Baseline evaluation\n",
        "            baseline_model, baseline_results = self.train_and_evaluate_baseline()\n",
        "\n",
        "            # 2. Enhanced evaluation\n",
        "            enhanced_model, enhanced_results = self.train_and_evaluate_enhanced()\n",
        "\n",
        "            # 3. SAHI evaluation\n",
        "            sahi_results = self.evaluate_with_sahi(\n",
        "                f'{self.base_dir}/enhanced_model/weights/best.pt'\n",
        "            )\n",
        "\n",
        "            # Function for safe metric extraction\n",
        "            def safe_get_metrics(results):\n",
        "                \"\"\"Safe metric extraction with fallbacks\"\"\"\n",
        "                if not hasattr(results, 'box'):\n",
        "                    logger.error(\"Validation results missing 'box' attribute\")\n",
        "                    return {\n",
        "                        'mAP50': 0,\n",
        "                        'mAP50-95': 0,\n",
        "                        'recall': 0,\n",
        "                        'precision': 0\n",
        "                    }\n",
        "\n",
        "                box = results.box\n",
        "                return {\n",
        "                    'mAP50': getattr(box, 'map50', 0),\n",
        "                    'mAP50-95': getattr(box, 'map', 0),\n",
        "                    'recall': getattr(box, 'r', 0),\n",
        "                    'precision': getattr(box, 'p', 0)\n",
        "                }\n",
        "\n",
        "            # 4. Collect all results\n",
        "            baseline_metrics = safe_get_metrics(baseline_results)\n",
        "            enhanced_metrics = safe_get_metrics(enhanced_results)\n",
        "\n",
        "            results_dict = {\n",
        "                'summary_metrics': {\n",
        "                    'Baseline': {\n",
        "                        'mAP50': baseline_metrics['mAP50'],\n",
        "                        'mAP50-95': baseline_metrics['mAP50-95'],\n",
        "                        'recall': baseline_metrics['recall']\n",
        "                    },\n",
        "                    'Enhanced+TTA': {\n",
        "                        'mAP50': enhanced_metrics['mAP50'],\n",
        "                        'mAP50-95': enhanced_metrics['mAP50-95'],\n",
        "                        'recall': enhanced_metrics['recall']\n",
        "                    }\n",
        "                },\n",
        "                'confusion_matrices': {\n",
        "                    'Baseline': baseline_results.confusion_matrix,\n",
        "                    'Enhanced+TTA': enhanced_results.confusion_matrix\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # 5. Generate visualizations\n",
        "            self.visualizer.generate_all_visualizations(\n",
        "                results_dict,\n",
        "                self.class_names\n",
        "            )\n",
        "\n",
        "            logger.info(\"Evaluation completed successfully!\")\n",
        "            return results_dict\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during evaluation: {str(e)}\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    def test_metrics_extraction(self):\n",
        "        \"\"\"Test metrics extraction works with current YOLO version\"\"\"\n",
        "        from unittest.mock import MagicMock\n",
        "\n",
        "        # Create mock results object\n",
        "        mock_results = MagicMock()\n",
        "        mock_results.box = MagicMock()\n",
        "        mock_results.box.map50 = 0.5\n",
        "        mock_results.box.map = 0.4\n",
        "        mock_results.box.r = 0.3\n",
        "        mock_results.box.p = 0.6\n",
        "        mock_results.confusion_matrix = None\n",
        "\n",
        "        # Since safe_get_metrics is inside run_complete_evaluation, redefining it here\n",
        "        def safe_get_metrics(results):\n",
        "            if not hasattr(results, 'box'):\n",
        "                return {\n",
        "                    'mAP50': 0,\n",
        "                    'mAP50-95': 0,\n",
        "                    'recall': 0,\n",
        "                    'precision': 0\n",
        "                }\n",
        "\n",
        "            box = results.box\n",
        "            return {\n",
        "                'mAP50': getattr(box, 'map50', 0),\n",
        "                'mAP50-95': getattr(box, 'map', 0),\n",
        "                'recall': getattr(box, 'r', 0),\n",
        "                'precision': getattr(box, 'p', 0)\n",
        "            }\n",
        "\n",
        "        metrics = safe_get_metrics(mock_results)\n",
        "        assert metrics['mAP50'] == 0.5\n",
        "        assert metrics['recall'] == 0.3\n",
        "        logger.info(\"✅ Metrics extraction test passed!\")\n",
        "\n",
        "        # Test missing attribute handling\n",
        "        mock_results.box = None\n",
        "        metrics = safe_get_metrics(mock_results)\n",
        "        assert metrics['mAP50'] == 0\n",
        "        logger.info(\"✅ Error handling test passed!\")"
      ],
      "metadata": {
        "id": "SbtaCT3WQoA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Main Pipeline\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = ModelEvaluator(BASE_DIR, DATA_YAML, DATASET_DIR)\n",
        "evaluator.test_metrics_extraction()\n",
        "\n",
        "# Run evaluation\n",
        "results = evaluator.run_complete_evaluation()\n",
        "\n",
        "logger.info(f\"Results saved in: {evaluator.visualizer.results_dir}\")"
      ],
      "metadata": {
        "id": "Vd6Z6HEmQtUY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "454da616-6375-4954-c8f7-2432d9189d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ModelEvaluator.__init__() takes 3 positional arguments but 4 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-49616e6912f4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Initialize evaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_YAML\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATASET_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_metrics_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ModelEvaluator.__init__() takes 3 positional arguments but 4 were given"
          ]
        }
      ]
    }
  ]
}