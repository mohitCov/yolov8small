{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcGG5aO2M2-5",
        "outputId": "6fbd4ab7-ad97-4c78-a08f-5bc054178e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 1. Initial Setup and Imports\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install ultralytics sahi pycocotools pandas seaborn --quiet\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.patches as patches\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from ultralytics import __version__ as yolo_version\n",
        "\n",
        "# First setup the logger\n",
        "class CustomFormatter(logging.Formatter):\n",
        "    \"\"\"Custom formatter with colors\"\"\"\n",
        "    grey = \"\\x1b[38;21m\"\n",
        "    blue = \"\\x1b[38;5;39m\"\n",
        "    yellow = \"\\x1b[38;5;226m\"\n",
        "    red = \"\\x1b[38;5;196m\"\n",
        "    reset = \"\\x1b[0m\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "                        datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
        "        self.FORMATS = {\n",
        "            logging.DEBUG: self.grey,\n",
        "            logging.INFO: self.blue,\n",
        "            logging.WARNING: self.yellow,\n",
        "            logging.ERROR: self.red\n",
        "        }\n",
        "\n",
        "    def format(self, record):\n",
        "        color = self.FORMATS.get(record.levelno, self.grey)\n",
        "        log_fmt = f\"{color}%(asctime)s - %(levelname)s - %(message)s{self.reset}\"\n",
        "        formatter = logging.Formatter(log_fmt, datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
        "        return formatter.format(record)\n",
        "\n",
        "# Setup logger (must come before any logger.info() calls)\n",
        "logger = logging.getLogger(\"YOLOComparison\")\n",
        "logger.setLevel(logging.INFO)\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setFormatter(CustomFormatter())\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "# Now you can use the logger\n",
        "logger.info(f\"Using Ultralytics YOLO version: {yolo_version}\")\n",
        "\n",
        "# Version compatibility check\n",
        "if yolo_version < '8.0.0':\n",
        "    logger.warning(\"This code was tested with YOLOv8.0.0+, older versions may need adjustments\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EbiqgBjVN15l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d99d20d-a291-4718-9961-dac2d75c0796"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[38;5;39m2025-06-06 18:37:19 - INFO - Using Ultralytics YOLO version: 8.3.151\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:37:19 - INFO - Using Ultralytics YOLO version: 8.3.151\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:37:19 - INFO - Using Ultralytics YOLO version: 8.3.151\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:37:19 - INFO - Using Ultralytics YOLO version: 8.3.151\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:37:19 - INFO - Using Ultralytics YOLO version: 8.3.151\u001b[0m\n",
            "INFO:YOLOComparison:Using Ultralytics YOLO version: 8.3.151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Configuration\n",
        "BASE_DIR = '/content/drive/MyDrive/new scope model'\n",
        "DATA_YAML = f'{BASE_DIR}/data.yaml'\n",
        "DATASET_DIR = '/content/drive/MyDrive/Samplesmall_dataset'"
      ],
      "metadata": {
        "id": "WKaBP6IbOVFV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Setup Logging\n",
        "class CustomFormatter(logging.Formatter):\n",
        "    \"\"\"Custom formatter with colors\"\"\"\n",
        "    grey = \"\\x1b[38;21m\"\n",
        "    blue = \"\\x1b[38;5;39m\"\n",
        "    yellow = \"\\x1b[38;5;226m\"\n",
        "    red = \"\\x1b[38;5;196m\"\n",
        "    reset = \"\\x1b[0m\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(fmt=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "                        datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
        "        self.FORMATS = {\n",
        "            logging.DEBUG: self.grey,\n",
        "            logging.INFO: self.blue,\n",
        "            logging.WARNING: self.yellow,\n",
        "            logging.ERROR: self.red\n",
        "        }\n",
        "\n",
        "    def format(self, record):\n",
        "        color = self.FORMATS.get(record.levelno, self.grey)\n",
        "        log_fmt = f\"{color}%(asctime)s - %(levelname)s - %(message)s{self.reset}\"\n",
        "        formatter = logging.Formatter(log_fmt, datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
        "        return formatter.format(record)\n",
        "\n",
        "# Setup logger\n",
        "logger = logging.getLogger(\"YOLOComparison\")\n",
        "logger.setLevel(logging.INFO)\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setFormatter(CustomFormatter())\n",
        "logger.addHandler(console_handler)\n"
      ],
      "metadata": {
        "id": "n3RjU2fXOfNt"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Visualization Class\n",
        "class ResultsVisualizer:\n",
        "    \"\"\"Handles all visualization tasks\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir):\n",
        "        self.base_dir = base_dir\n",
        "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Create organized directory structure\n",
        "        self.results_dir = os.path.join(base_dir, f'comparison_results_{self.timestamp}')\n",
        "        self.dirs = {\n",
        "            'plots': os.path.join(self.results_dir, 'plots'),\n",
        "            'metrics': os.path.join(self.results_dir, 'metrics'),\n",
        "            'detections': os.path.join(self.results_dir, 'detection_examples'),\n",
        "            'logs': os.path.join(self.results_dir, 'logs')\n",
        "        }\n",
        "\n",
        "        # Create all directories\n",
        "        for dir_path in self.dirs.values():\n",
        "            os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "        # Setup file logging\n",
        "        file_handler = logging.FileHandler(os.path.join(self.dirs['logs'], 'evaluation.log'))\n",
        "        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
        "        logger.addHandler(file_handler)\n",
        "\n",
        "        logger.info(f\"Created results directory at {self.results_dir}\")"
      ],
      "metadata": {
        "id": "9oIXUAaXO1Lz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.patches as patches\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "Zctl3l-4P0IN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelEvaluator:\n",
        "    def __init__(self, base_dir, data_yaml):\n",
        "        self.base_dir = base_dir\n",
        "        self.data_yaml = data_yaml\n",
        "        self.visualizer = ResultsVisualizer(base_dir)\n",
        "\n",
        "        # Load class names from yaml\n",
        "        with open(data_yaml, 'r') as f:\n",
        "            self.data_config = yaml.safe_load(f)\n",
        "        self.class_names = self.data_config['names']\n",
        "\n",
        "    def collect_evaluation_data(self, baseline_results, enhanced_results, sahi_results):\n",
        "        \"\"\"Collect all evaluation data for visualization\"\"\"\n",
        "\n",
        "        # Collect metrics for each model\n",
        "        results_dict = {\n",
        "            'summary_metrics': {\n",
        "                'Baseline': {\n",
        "                    'mAP50': baseline_results.box.map50,\n",
        "                    'mAP50-95': baseline_results.box.map,\n",
        "                    'Mean Precision': baseline_results.box.mp(),  # Using mp() method\n",
        "                    'Mean Recall': baseline_results.box.mr()  # Using mr() method\n",
        "                },\n",
        "                'Enhanced+TTA': {\n",
        "                    'mAP50': enhanced_results.box.map50,\n",
        "                    'mAP50-95': enhanced_results.box.map,\n",
        "                    'Mean Precision': enhanced_results.box.mp(),  # Using mp() method\n",
        "                    'Mean Recall': enhanced_results.box.mr()  # Using mr() method\n",
        "                }\n",
        "            },\n",
        "            'confusion_matrices': {\n",
        "                'Baseline': {\n",
        "                    'true': baseline_results.box.confusion_matrix.matrix.T,  # Transpose for correct orientation\n",
        "                    'pred': range(len(self.class_names))\n",
        "                },\n",
        "                'Enhanced+TTA': {\n",
        "                    'true': enhanced_results.box.confusion_matrix.matrix.T,  # Transpose for correct orientation\n",
        "                    'pred': range(len(self.class_names))\n",
        "                }\n",
        "            },\n",
        "            'precision': [\n",
        "                baseline_results.box.p,  # Using precision attribute 'p'\n",
        "                enhanced_results.box.p\n",
        "            ],\n",
        "            'recall': [\n",
        "                baseline_results.box.r,  # Using recall attribute 'r'\n",
        "                enhanced_results.box.r\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        def get_metrics(results):\n",
        "            \"\"\"Safe metric extraction with fallbacks\"\"\"\n",
        "            metrics = {\n",
        "                'mAP50': getattr(results.box, 'map50', 0),\n",
        "                'mAP50-95': getattr(results.box, 'map', 0),\n",
        "                'recall': getattr(results.box, 'r', getattr(results.box, 'recall', 0)),\n",
        "                'precision': getattr(results.box, 'p', 0)\n",
        "            }\n",
        "            return metrics\n",
        "\n",
        "        # Then use it like this:\n",
        "        baseline_metrics = get_metrics(baseline_results)\n",
        "        enhanced_metrics = get_metrics(enhanced_results)\n"
      ],
      "metadata": {
        "id": "_A02-qaoP7NW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def __init__(self, base_dir):\n",
        "        self.base_dir = base_dir\n",
        "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Create organized directory structure\n",
        "        self.results_dir = os.path.join(base_dir, f'comparison_results_{self.timestamp}')\n",
        "        self.dirs = {\n",
        "            'plots': os.path.join(self.results_dir, 'plots'),\n",
        "            'metrics': os.path.join(self.results_dir, 'metrics'),\n",
        "            'detections': os.path.join(self.results_dir, 'detection_examples'),\n",
        "            'logs': os.path.join(self.results_dir, 'logs')\n",
        "        }\n",
        "\n",
        "        # Create all directories\n",
        "        for dir_path in self.dirs.values():\n",
        "            os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "        # Setup file logging\n",
        "        file_handler = logging.FileHandler(os.path.join(self.dirs['logs'], 'evaluation.log'))\n",
        "        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
        "        logger.addHandler(file_handler)\n",
        "\n",
        "        logger.info(f\"Created results directory at {self.results_dir}\")"
      ],
      "metadata": {
        "id": "q1p0armdQ3jE"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResultsVisualizer:\n",
        "    \"\"\"Handles all visualization tasks\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir):\n",
        "        self.base_dir = base_dir\n",
        "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Create organized directory structure\n",
        "        self.results_dir = os.path.join(base_dir, f'comparison_results_{self.timestamp}')\n",
        "        self.dirs = {\n",
        "            'plots': os.path.join(self.results_dir, 'plots'),\n",
        "            'metrics': os.path.join(self.results_dir, 'metrics'),\n",
        "            'detections': os.path.join(self.results_dir, 'detection_examples'),\n",
        "            'logs': os.path.join(self.results_dir, 'logs')\n",
        "        }\n",
        "\n",
        "        # Create all directories\n",
        "        for dir_path in self.dirs.values():\n",
        "            os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "        logger.info(f\"Created results directory structure at {self.results_dir}\")\n",
        "\n",
        "    def plot_metrics_comparison(self, metrics_dict):\n",
        "        \"\"\"Create grouped bar plot for metrics comparison\"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        df = pd.DataFrame(metrics_dict).T\n",
        "\n",
        "        # Create grouped bar plot\n",
        "        ax = df.plot(kind='bar', width=0.8)\n",
        "        plt.title('Performance Comparison Across Models')\n",
        "        plt.xlabel('Model Type')\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        # Add value labels\n",
        "        for container in ax.containers:\n",
        "            ax.bar_label(container, fmt='%.3f')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(self.dirs['plots'], 'metrics_comparison.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Save metrics to CSV\n",
        "        csv_path = os.path.join(self.dirs['metrics'], 'metrics_comparison.csv')\n",
        "        df.to_csv(csv_path)\n",
        "\n",
        "        # Create markdown table\n",
        "        markdown_path = os.path.join(self.dirs['metrics'], 'metrics_summary.md')\n",
        "        with open(markdown_path, 'w') as f:\n",
        "            f.write(\"# Model Performance Comparison\\n\\n\")\n",
        "            f.write(df.to_markdown())\n",
        "\n",
        "        return df\n",
        "\n",
        "    def plot_confusion_matrix(self, true_labels, pred_labels, class_names, model_name):\n",
        "        \"\"\"Plot confusion matrix for each model\"\"\"\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        cm = confusion_matrix(true_labels, pred_labels)\n",
        "        sns.heatmap(cm, annot=True, fmt='d',\n",
        "                   xticklabels=class_names,\n",
        "                   yticklabels=class_names)\n",
        "        plt.title(f'Confusion Matrix - {model_name}')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "\n",
        "        save_path = os.path.join(self.dirs['plots'], f'confusion_matrix_{model_name}.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_precision_recall_curves(self, precisions, recalls, model_names):\n",
        "        \"\"\"Plot precision-recall curves for all models\"\"\"\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for i, model_name in enumerate(model_names):\n",
        "            plt.plot(recalls[i], precisions[i], label=model_name)\n",
        "\n",
        "        plt.title('Precision-Recall Curves')\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        save_path = os.path.join(self.dirs['plots'], 'precision_recall_curves.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_per_class_map(self, class_maps, class_names, model_names):\n",
        "        \"\"\"Plot per-class mAP comparison\"\"\"\n",
        "        df = pd.DataFrame(class_maps, index=model_names, columns=class_names)\n",
        "\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        ax = df.plot(kind='bar', width=0.8)\n",
        "        plt.title('Per-Class mAP Comparison')\n",
        "        plt.xlabel('Model')\n",
        "        plt.ylabel('mAP')\n",
        "        plt.legend(title='Classes', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "        # Add value labels\n",
        "        for container in ax.containers:\n",
        "            ax.bar_label(container, fmt='%.3f', rotation=90)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(self.dirs['plots'], 'per_class_map.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def create_detection_grid(self, images, detections, class_names, model_names):\n",
        "        \"\"\"Create a grid of detection examples\"\"\"\n",
        "        n_images = len(images)\n",
        "        n_models = len(model_names)\n",
        "\n",
        "        fig, axes = plt.subplots(n_images, n_models,\n",
        "                                figsize=(5*n_models, 5*n_images))\n",
        "\n",
        "        for i in range(n_images):\n",
        "            for j in range(n_models):\n",
        "                if n_images == 1:\n",
        "                    ax = axes[j]\n",
        "                else:\n",
        "                    ax = axes[i, j]\n",
        "\n",
        "                ax.imshow(images[i])\n",
        "\n",
        "                # Plot detections\n",
        "                for det in detections[j][i]:\n",
        "                    bbox = det['bbox']\n",
        "                    label = class_names[det['class_id']]\n",
        "                    conf = det['confidence']\n",
        "\n",
        "                    rect = patches.Rectangle(\n",
        "                        (bbox[0], bbox[1]), bbox[2], bbox[3],\n",
        "                        linewidth=2, edgecolor='r', facecolor='none'\n",
        "                    )\n",
        "                    ax.add_patch(rect)\n",
        "                    ax.text(bbox[0], bbox[1]-5,\n",
        "                           f'{label}: {conf:.2f}',\n",
        "                           color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
        "\n",
        "                if i == 0:\n",
        "                    ax.set_title(model_names[j])\n",
        "                ax.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(self.dirs['detections'], 'detection_grid.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def save_markdown_report(self, metrics_df, additional_notes=None):\n",
        "        \"\"\"Create a comprehensive markdown report\"\"\"\n",
        "        report_path = os.path.join(self.dirs['metrics'], 'complete_report.md')\n",
        "\n",
        "        with open(report_path, 'w') as f:\n",
        "            f.write(\"# Model Comparison Report\\n\\n\")\n",
        "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "            f.write(\"## Summary Metrics\\n\")\n",
        "            f.write(metrics_df.to_markdown())\n",
        "            f.write(\"\\n\\n\")\n",
        "\n",
        "            f.write(\"## Visualization Directory Structure\\n\")\n",
        "            for dir_name, dir_path in self.dirs.items():\n",
        "                f.write(f\"- {dir_name}: {dir_path}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            if additional_notes:\n",
        "                f.write(\"## Additional Notes\\n\")\n",
        "                f.write(additional_notes)\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            f.write(\"\\n## Plots Generated\\n\")\n",
        "            f.write(\"1. Metrics Comparison (Bar Plot)\\n\")\n",
        "            f.write(\"2. Confusion Matrices\\n\")\n",
        "            f.write(\"3. Precision-Recall Curves\\n\")\n",
        "            f.write(\"4. Per-Class mAP Comparison\\n\")\n",
        "            f.write(\"5. Detection Examples Grid\\n\")\n",
        "\n",
        "def generate_all_visualizations(results_dict, class_names, base_dir):\n",
        "    \"\"\"Main function to generate all visualizations\"\"\"\n",
        "    visualizer = ResultsVisualizer(base_dir)\n",
        "\n",
        "    # 1. Overall metrics comparison\n",
        "    metrics_df = visualizer.plot_metrics_comparison(results_dict['summary_metrics'])\n",
        "\n",
        "    # 2. Confusion matrices\n",
        "    for model_name in results_dict['confusion_matrices']:\n",
        "        visualizer.plot_confusion_matrix(\n",
        "            results_dict['confusion_matrices'][model_name]['true'],\n",
        "            results_dict['confusion_matrices'][model_name]['pred'],\n",
        "            class_names,\n",
        "            model_name\n",
        "        )\n",
        "\n",
        "    # 3. Precision-recall curves\n",
        "    visualizer.plot_precision_recall_curves(\n",
        "        results_dict['precision'],\n",
        "        results_dict['recall'],\n",
        "        list(results_dict['summary_metrics'].keys())\n",
        "    )\n",
        "\n",
        "    # 4. Per-class mAP\n",
        "    visualizer.plot_per_class_map(\n",
        "        results_dict['per_class_map'],\n",
        "        class_names,\n",
        "        list(results_dict['summary_metrics'].keys())\n",
        "    )\n",
        "\n",
        "    # 5. Detection grid\n",
        "    visualizer.create_detection_grid(\n",
        "        results_dict['example_images'],\n",
        "        results_dict['example_detections'],\n",
        "        class_names,\n",
        "        list(results_dict['summary_metrics'].keys())\n",
        "    )\n",
        "\n",
        "    # 6. Generate markdown report\n",
        "    visualizer.save_markdown_report(\n",
        "        metrics_df,\n",
        "        additional_notes=results_dict.get('notes', None)\n",
        "    )\n",
        "\n",
        "    logger.info(f\"All visualizations saved in {visualizer.results_dir}\")\n",
        "    return visualizer.results_dir"
      ],
      "metadata": {
        "id": "qCOS6S_oQciL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelEvaluator:\n",
        "    def __init__(self, base_dir, data_yaml):\n",
        "        self.base_dir = base_dir\n",
        "        self.data_yaml = data_yaml\n",
        "        self.visualizer = ResultsVisualizer(base_dir)\n",
        "\n",
        "        # Load class names from yaml\n",
        "        with open(data_yaml, 'r') as f:\n",
        "            self.data_config = yaml.safe_load(f)\n",
        "        self.class_names = self.data_config['names']\n",
        "\n",
        "        logger.info(f\"Initialized evaluator with {len(self.class_names)} classes\")\n",
        "\n",
        "    def train_and_evaluate_baseline(self):\n",
        "        \"\"\"Train and evaluate baseline YOLOv8 model\"\"\"\n",
        "        logger.info(\"Starting baseline model training...\")\n",
        "\n",
        "        model = YOLO('yolov8n.pt')\n",
        "        results = model.train(\n",
        "            data=self.data_yaml,\n",
        "            epochs=2,\n",
        "            imgsz=640,\n",
        "            project=self.base_dir,\n",
        "            name='baseline_model'\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        val_results = model.val(data=self.data_yaml)\n",
        "        return model, val_results\n",
        "\n",
        "    def train_and_evaluate_enhanced(self):\n",
        "        \"\"\"Train and evaluate enhanced model (larger size + TTA)\"\"\"\n",
        "        logger.info(\"Starting enhanced model training...\")\n",
        "\n",
        "        model = YOLO('yolov8n.pt')\n",
        "        results = model.train(\n",
        "            data=self.data_yaml,\n",
        "            epochs=2,\n",
        "            imgsz=1024,\n",
        "            project=self.base_dir,\n",
        "            name='enhanced_model'\n",
        "        )\n",
        "\n",
        "        # Evaluate with TTA\n",
        "        val_results = model.val(\n",
        "            data=self.data_yaml,\n",
        "            imgsz=1024,\n",
        "            augment=True\n",
        "        )\n",
        "        return model, val_results\n",
        "\n",
        "    def evaluate_with_sahi(self, model_path):\n",
        "        \"\"\"Evaluate using SAHI with better error handling\"\"\"\n",
        "        try:\n",
        "            from sahi import AutoDetectionModel\n",
        "            from sahi.predict import get_sliced_prediction\n",
        "\n",
        "            logger.info(f\"Loading model from {model_path}\")\n",
        "            if not os.path.exists(model_path):\n",
        "                raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "\n",
        "            detection_model = AutoDetectionModel.from_pretrained(\n",
        "                model_type='ultralytics',\n",
        "                model_path=model_path,\n",
        "                confidence_threshold=0.3,\n",
        "                device='cuda'\n",
        "            )\n",
        "\n",
        "            test_images_dir = os.path.join(DATASET_DIR, 'test/images')\n",
        "            if not os.path.exists(test_images_dir):\n",
        "                raise FileNotFoundError(f\"Test images dir not found: {test_images_dir}\")\n",
        "\n",
        "            results = []\n",
        "            for image_name in os.listdir(test_images_dir):\n",
        "                if image_name.endswith(('.jpg', '.png')):\n",
        "                    image_path = os.path.join(test_images_dir, image_name)\n",
        "                    try:\n",
        "                        result = get_sliced_prediction(\n",
        "                            image=image_path,\n",
        "                            detection_model=detection_model,\n",
        "                            slice_height=512,\n",
        "                            slice_width=512,\n",
        "                            overlap_height_ratio=0.2,\n",
        "                            overlap_width_ratio=0.2\n",
        "                        )\n",
        "                        results.append(result)\n",
        "                        logger.debug(f\"Processed {image_name} successfully\")\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Failed to process {image_name}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"SAHI evaluation failed: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def run_complete_evaluation(self):\n",
        "        \"\"\"Run complete evaluation pipeline\"\"\"\n",
        "        try:\n",
        "            # 1. Baseline evaluation\n",
        "            baseline_model, baseline_results = self.train_and_evaluate_baseline()\n",
        "\n",
        "            # 2. Enhanced evaluation\n",
        "            enhanced_model, enhanced_results = self.train_and_evaluate_enhanced()\n",
        "\n",
        "            # 3. SAHI evaluation\n",
        "            sahi_results = self.evaluate_with_sahi(\n",
        "                f'{self.base_dir}/enhanced_model/weights/best.pt'\n",
        "            )\n",
        "\n",
        "            # Function for safe metric extraction\n",
        "            def safe_get_metrics(results):\n",
        "                \"\"\"Safe metric extraction with fallbacks\"\"\"\n",
        "                if not hasattr(results, 'box'):\n",
        "                    logger.error(\"Validation results missing 'box' attribute\")\n",
        "                    return {\n",
        "                        'mAP50': 0,\n",
        "                        'mAP50-95': 0,\n",
        "                        'recall': 0,\n",
        "                        'precision': 0\n",
        "                    }\n",
        "\n",
        "                box = results.box\n",
        "                return {\n",
        "                    'mAP50': getattr(box, 'map50', 0),\n",
        "                    'mAP50-95': getattr(box, 'map', 0),\n",
        "                    'recall': getattr(box, 'r', 0),\n",
        "                    'precision': getattr(box, 'p', 0)\n",
        "                }\n",
        "\n",
        "            # 4. Collect all results\n",
        "            baseline_metrics = safe_get_metrics(baseline_results)\n",
        "            enhanced_metrics = safe_get_metrics(enhanced_results)\n",
        "\n",
        "            results_dict = {\n",
        "                'summary_metrics': {\n",
        "                    'Baseline': {\n",
        "                        'mAP50': baseline_metrics['mAP50'],\n",
        "                        'mAP50-95': baseline_metrics['mAP50-95'],\n",
        "                        'recall': baseline_metrics['recall']\n",
        "                    },\n",
        "                    'Enhanced+TTA': {\n",
        "                        'mAP50': enhanced_metrics['mAP50'],\n",
        "                        'mAP50-95': enhanced_metrics['mAP50-95'],\n",
        "                        'recall': enhanced_metrics['recall']\n",
        "                    }\n",
        "                },\n",
        "                'confusion_matrices': {\n",
        "                    'Baseline': baseline_results.confusion_matrix,\n",
        "                    'Enhanced+TTA': enhanced_results.confusion_matrix\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # 5. Generate visualizations\n",
        "            self.visualizer.generate_all_visualizations(\n",
        "                results_dict,\n",
        "                self.class_names\n",
        "            )\n",
        "\n",
        "            logger.info(\"Evaluation completed successfully!\")\n",
        "            return results_dict\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during evaluation: {str(e)}\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    def test_metrics_extraction(self):\n",
        "        \"\"\"Test metrics extraction works with current YOLO version\"\"\"\n",
        "        from unittest.mock import MagicMock\n",
        "\n",
        "        # Create mock results object\n",
        "        mock_results = MagicMock()\n",
        "        mock_results.box = MagicMock()\n",
        "        mock_results.box.map50 = 0.5\n",
        "        mock_results.box.map = 0.4\n",
        "        mock_results.box.r = 0.3\n",
        "        mock_results.box.p = 0.6\n",
        "        mock_results.confusion_matrix = None\n",
        "\n",
        "        # Since safe_get_metrics is inside run_complete_evaluation, redefining it here\n",
        "        def safe_get_metrics(results):\n",
        "            if not hasattr(results, 'box'):\n",
        "                return {\n",
        "                    'mAP50': 0,\n",
        "                    'mAP50-95': 0,\n",
        "                    'recall': 0,\n",
        "                    'precision': 0\n",
        "                }\n",
        "\n",
        "            box = results.box\n",
        "            return {\n",
        "                'mAP50': getattr(box, 'map50', 0),\n",
        "                'mAP50-95': getattr(box, 'map', 0),\n",
        "                'recall': getattr(box, 'r', 0),\n",
        "                'precision': getattr(box, 'p', 0)\n",
        "            }\n",
        "\n",
        "        metrics = safe_get_metrics(mock_results)\n",
        "        assert metrics['mAP50'] == 0.5\n",
        "        assert metrics['recall'] == 0.3\n",
        "        logger.info(\"✅ Metrics extraction test passed!\")\n",
        "\n",
        "        # Test missing attribute handling\n",
        "        mock_results.box = None\n",
        "        metrics = safe_get_metrics(mock_results)\n",
        "        assert metrics['mAP50'] == 0\n",
        "        logger.info(\"✅ Error handling test passed!\")\n"
      ],
      "metadata": {
        "id": "4htUa7q9Qsyz"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    logger.info(\"Starting YOLOv8 comparison pipeline...\")\n",
        "\n",
        "    # Verify paths\n",
        "    for path in [BASE_DIR, DATA_YAML, DATASET_DIR]:\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(f\"Path not found: {path}\")\n",
        "\n",
        "    # Initialize evaluator\n",
        "    evaluator = ModelEvaluator(BASE_DIR, DATA_YAML)\n",
        "    evaluator.test_metrics_extraction()\n",
        "\n",
        "    # Run evaluation\n",
        "    results = evaluator.run_complete_evaluation()\n",
        "\n",
        "    logger.info(f\"Results saved in: {evaluator.visualizer.results_dir}\")\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Run the evaluation\n",
        "    results = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "6Z0l0AGHSWC9",
        "outputId": "bf31445b-2fb5-4ff4-d1cc-d54c38c8cc98",
        "collapsed": true
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-0149d9bcf6e6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Run the evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "class ResultsVisualizer:\n",
        "    \"\"\"Handles all visualization tasks\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir):\n",
        "        self.base_dir = base_dir\n",
        "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Create organized directory structure\n",
        "        self.results_dir = os.path.join(base_dir, f'comparison_results_{self.timestamp}')\n",
        "        self.dirs = {\n",
        "            'plots': os.path.join(self.results_dir, 'plots'),\n",
        "            'metrics': os.path.join(self.results_dir, 'metrics'),\n",
        "            'detections': os.path.join(self.results_dir, 'detection_examples'),\n",
        "            'logs': os.path.join(self.results_dir, 'logs')\n",
        "        }\n",
        "\n",
        "        # Create all directories\n",
        "        for dir_path in self.dirs.values():\n",
        "            os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "        # Setup file logging\n",
        "        file_handler = logging.FileHandler(os.path.join(self.dirs['logs'], 'evaluation.log'))\n",
        "        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
        "        logger.addHandler(file_handler)\n",
        "\n",
        "        logger.info(f\"Created results directory at {self.results_dir}\")\n",
        "\n",
        "    def plot_metrics_comparison(self, metrics_dict):\n",
        "        \"\"\"Create grouped bar plot for metrics comparison\"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        df = pd.DataFrame(metrics_dict).T\n",
        "\n",
        "        ax = df.plot(kind='bar', width=0.8)\n",
        "        plt.title('Performance Comparison Across Models')\n",
        "        plt.xlabel('Model Type')\n",
        "        plt.ylabel('Score')\n",
        "\n",
        "        for container in ax.containers:\n",
        "            ax.bar_label(container, fmt='%.3f')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(self.dirs['plots'], 'metrics_comparison.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        csv_path = os.path.join(self.dirs['metrics'], 'metrics_comparison.csv')\n",
        "        df.to_csv(csv_path)\n",
        "\n",
        "        markdown_path = os.path.join(self.dirs['metrics'], 'metrics_summary.md')\n",
        "        with open(markdown_path, 'w') as f:\n",
        "            f.write(\"# Model Performance Comparison\\n\\n\")\n",
        "            f.write(df.to_markdown())\n",
        "\n",
        "        return df\n",
        "\n",
        "    def plot_confusion_matrix(self, true_labels, pred_labels, class_names, model_name):\n",
        "        \"\"\"Plot confusion matrix for each model\"\"\"\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        cm = confusion_matrix(true_labels, pred_labels)\n",
        "        sns.heatmap(cm, annot=True, fmt='d',\n",
        "                    xticklabels=class_names,\n",
        "                    yticklabels=class_names)\n",
        "        plt.title(f'Confusion Matrix - {model_name}')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "\n",
        "        save_path = os.path.join(self.dirs['plots'], f'confusion_matrix_{model_name}.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_precision_recall_curves(self, precisions, recalls, model_names):\n",
        "        \"\"\"Plot precision-recall curves for all models\"\"\"\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for i, model_name in enumerate(model_names):\n",
        "            plt.plot(recalls[i], precisions[i], label=model_name)\n",
        "\n",
        "        plt.title('Precision-Recall Curves')\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        save_path = os.path.join(self.dirs['plots'], 'precision_recall_curves.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_per_class_map(self, class_maps, class_names, model_names):\n",
        "        \"\"\"Plot per-class mAP comparison\"\"\"\n",
        "        df = pd.DataFrame(class_maps, index=model_names, columns=class_names)\n",
        "\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        ax = df.plot(kind='bar', width=0.8)\n",
        "        plt.title('Per-Class mAP Comparison')\n",
        "        plt.xlabel('Model')\n",
        "        plt.ylabel('mAP')\n",
        "        plt.legend(title='Classes', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "        for container in ax.containers:\n",
        "            ax.bar_label(container, fmt='%.3f', rotation=90)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(self.dirs['plots'], 'per_class_map.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def create_detection_grid(self, images, detections, class_names, model_names):\n",
        "        \"\"\"Create a grid of detection examples\"\"\"\n",
        "        n_images = len(images)\n",
        "        n_models = len(model_names)\n",
        "\n",
        "        fig, axes = plt.subplots(n_images, n_models, figsize=(5*n_models, 5*n_images))\n",
        "\n",
        "        for i in range(n_images):\n",
        "            for j in range(n_models):\n",
        "                if n_images == 1:\n",
        "                    ax = axes[j]\n",
        "                else:\n",
        "                    ax = axes[i, j]\n",
        "\n",
        "                ax.imshow(images[i])\n",
        "\n",
        "                for det in detections[j][i]:\n",
        "                    bbox = det['bbox']\n",
        "                    label = class_names[det['class_id']]\n",
        "                    conf = det['confidence']\n",
        "\n",
        "                    rect = patches.Rectangle(\n",
        "                        (bbox[0], bbox[1]), bbox[2], bbox[3],\n",
        "                        linewidth=2, edgecolor='r', facecolor='none'\n",
        "                    )\n",
        "                    ax.add_patch(rect)\n",
        "                    ax.text(bbox[0], bbox[1]-5,\n",
        "                            f'{label}: {conf:.2f}',\n",
        "                            color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
        "\n",
        "                if i == 0:\n",
        "                    ax.set_title(model_names[j])\n",
        "                ax.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(self.dirs['detections'], 'detection_grid.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def save_markdown_report(self, metrics_df, additional_notes=None):\n",
        "        \"\"\"Create a comprehensive markdown report\"\"\"\n",
        "        report_path = os.path.join(self.dirs['metrics'], 'complete_report.md')\n",
        "\n",
        "        with open(report_path, 'w') as f:\n",
        "            f.write(\"# Model Comparison Report\\n\\n\")\n",
        "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "            f.write(\"## Summary Metrics\\n\")\n",
        "            f.write(metrics_df.to_markdown())\n",
        "            f.write(\"\\n\\n\")\n",
        "\n",
        "            f.write(\"## Visualization Directory Structure\\n\")\n",
        "            for dir_name, dir_path in self.dirs.items():\n",
        "                f.write(f\"- {dir_name}: {dir_path}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            if additional_notes:\n",
        "                f.write(\"## Additional Notes\\n\")\n",
        "                f.write(additional_notes)\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            f.write(\"\\n## Plots Generated\\n\")\n",
        "            f.write(\"1. Metrics Comparison (Bar Plot)\\n\")\n",
        "            f.write(\"2. Confusion Matrices\\n\")\n",
        "            f.write(\"3. Precision-Recall Curves\\n\")\n",
        "            f.write(\"4. Per-Class mAP Comparison\\n\")\n",
        "            f.write(\"5. Detection Examples Grid\\n\")\n",
        "\n",
        "    def generate_all_visualizations(self, results_dict, class_names):\n",
        "        \"\"\"Main function to generate all visualizations\"\"\"\n",
        "\n",
        "        # 1. Overall metrics comparison\n",
        "        metrics_df = self.plot_metrics_comparison(results_dict['summary_metrics'])\n",
        "\n",
        "        # 2. Confusion matrices\n",
        "        for model_name, cm_object in results_dict['confusion_matrices'].items():\n",
        "            self.plot_confusion_matrix(\n",
        "                cm_object.y_true,\n",
        "                cm_object.y_pred,\n",
        "                class_names,\n",
        "                model_name\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # 3. Precision-recall curves\n",
        "        self.plot_precision_recall_curves(\n",
        "            results_dict['precision'],\n",
        "            results_dict['recall'],\n",
        "            list(results_dict['summary_metrics'].keys())\n",
        "        )\n",
        "\n",
        "        # 4. Per-class mAP\n",
        "        self.plot_per_class_map(\n",
        "            results_dict['per_class_map'],\n",
        "            class_names,\n",
        "            list(results_dict['summary_metrics'].keys())\n",
        "        )\n",
        "\n",
        "        # 5. Detection grid\n",
        "        self.create_detection_grid(\n",
        "            results_dict['example_images'],\n",
        "            results_dict['example_detections'],\n",
        "            class_names,\n",
        "            list(results_dict['summary_metrics'].keys())\n",
        "        )\n",
        "\n",
        "        # 6. Generate markdown report\n",
        "        self.save_markdown_report(\n",
        "            metrics_df,\n",
        "            additional_notes=results_dict.get('notes', None)\n",
        "        )\n",
        "\n",
        "        logger.info(f\"All visualizations saved in {self.results_dir}\")\n",
        "        return self.results_dir\n"
      ],
      "metadata": {
        "id": "hl4q8BU0eI7i"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    logger.info(\"Starting YOLOv8 comparison pipeline...\")\n",
        "\n",
        "    # Verify paths\n",
        "    for path in [BASE_DIR, DATA_YAML, DATASET_DIR]:\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(f\"Path not found: {path}\")\n",
        "\n",
        "    # Initialize evaluator\n",
        "    evaluator = ModelEvaluator(BASE_DIR, DATA_YAML)\n",
        "    evaluator.test_metrics_extraction()\n",
        "\n",
        "    # Run evaluation\n",
        "    results = evaluator.run_complete_evaluation()\n",
        "\n",
        "    logger.info(f\"Results saved in: {evaluator.visualizer.results_dir}\")\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Run the evaluation\n",
        "    results = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "qza7wNWFeONx",
        "outputId": "fd83dc12-81b4-474c-d0a0-627b9cb7d2a3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Starting YOLOv8 comparison pipeline...\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Starting YOLOv8 comparison pipeline...\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Starting YOLOv8 comparison pipeline...\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Starting YOLOv8 comparison pipeline...\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Starting YOLOv8 comparison pipeline...\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Starting YOLOv8 comparison pipeline...\u001b[0m\n",
            "INFO:YOLOComparison:Starting YOLOv8 comparison pipeline...\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Created results directory structure at /content/drive/MyDrive/new scope model/comparison_results_20250606_183812\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Created results directory structure at /content/drive/MyDrive/new scope model/comparison_results_20250606_183812\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Created results directory structure at /content/drive/MyDrive/new scope model/comparison_results_20250606_183812\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Created results directory structure at /content/drive/MyDrive/new scope model/comparison_results_20250606_183812\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Created results directory structure at /content/drive/MyDrive/new scope model/comparison_results_20250606_183812\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Created results directory structure at /content/drive/MyDrive/new scope model/comparison_results_20250606_183812\u001b[0m\n",
            "INFO:YOLOComparison:Created results directory structure at /content/drive/MyDrive/new scope model/comparison_results_20250606_183812\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Initialized evaluator with 8 classes\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Initialized evaluator with 8 classes\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Initialized evaluator with 8 classes\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Initialized evaluator with 8 classes\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Initialized evaluator with 8 classes\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Initialized evaluator with 8 classes\u001b[0m\n",
            "INFO:YOLOComparison:Initialized evaluator with 8 classes\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - ✅ Metrics extraction test passed!\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - ✅ Metrics extraction test passed!\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - ✅ Metrics extraction test passed!\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - ✅ Metrics extraction test passed!\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - ✅ Metrics extraction test passed!\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - ✅ Metrics extraction test passed!\u001b[0m\n",
            "INFO:YOLOComparison:✅ Metrics extraction test passed!\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - ✅ Error handling test passed!\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - ✅ Error handling test passed!\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - ✅ Error handling test passed!\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - ✅ Error handling test passed!\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - ✅ Error handling test passed!\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - ✅ Error handling test passed!\u001b[0m\n",
            "INFO:YOLOComparison:✅ Error handling test passed!\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Starting baseline model training...\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Starting baseline model training...\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Starting baseline model training...\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Starting baseline model training...\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Starting baseline model training...\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:12 - INFO - Starting baseline model training...\u001b[0m\n",
            "INFO:YOLOComparison:Starting baseline model training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Ultralytics 8.3.151 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/new scope model/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=2, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=baseline_model17, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/new scope model, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/new scope model/baseline_model17, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.5±0.3 ms, read: 128.2±63.7 MB/s, size: 818.4 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Samplesmall_dataset/train/labels.cache... 138 images, 54 backgrounds, 0 corrupt: 100%|██████████| 192/192 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.4±0.1 ms, read: 104.5±29.7 MB/s, size: 811.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Samplesmall_dataset/val/labels.cache... 37 images, 113 backgrounds, 0 corrupt: 100%|██████████| 150/150 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to /content/drive/MyDrive/new scope model/baseline_model17/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/new scope model/baseline_model17\u001b[0m\n",
            "Starting training for 2 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/2      2.19G      1.753      4.417       1.24         82        640: 100%|██████████| 12/12 [00:05<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  3.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        254   0.000402      0.215   0.000956   0.000253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/2      2.22G      1.827      3.515      1.218        119        640: 100%|██████████| 12/12 [00:05<00:00,  2.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        254   0.000715      0.214    0.00115   0.000337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2 epochs completed in 0.004 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/new scope model/baseline_model17/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/drive/MyDrive/new scope model/baseline_model17/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/drive/MyDrive/new scope model/baseline_model17/weights/best.pt...\n",
            "Ultralytics 8.3.151 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:01<00:00,  2.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        254   0.000715      0.214    0.00115   0.000338\n",
            "                   Car         24        122    0.00509      0.189    0.00636    0.00209\n",
            "            Pedestrian         37         93    0.00043     0.0215    0.00031   6.38e-05\n",
            "                   Van          8         11          0          0          0          0\n",
            "               Cyclist          8         14          0          0          0          0\n",
            "        Person_sitting          1          1   9.94e-05          1    0.00126   0.000377\n",
            "                  Misc          2          7          0          0          0          0\n",
            "                 Truck          3          4          0          0          0          0\n",
            "                  Tram          1          2   0.000101        0.5    0.00131   0.000176\n",
            "Speed: 0.2ms preprocess, 2.1ms inference, 0.0ms loss, 5.5ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/new scope model/baseline_model17\u001b[0m\n",
            "Ultralytics 8.3.151 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.5±0.3 ms, read: 184.2±67.4 MB/s, size: 840.5 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Samplesmall_dataset/val/labels.cache... 37 images, 113 backgrounds, 0 corrupt: 100%|██████████| 150/150 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  2.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        254   0.000766      0.216    0.00118   0.000341\n",
            "                   Car         24        122    0.00528      0.197    0.00639    0.00208\n",
            "            Pedestrian         37         93    0.00064     0.0323   0.000418   8.54e-05\n",
            "                   Van          8         11          0          0          0          0\n",
            "               Cyclist          8         14          0          0          0          0\n",
            "        Person_sitting          1          1   9.93e-05          1    0.00131   0.000394\n",
            "                  Misc          2          7          0          0          0          0\n",
            "                 Truck          3          4          0          0          0          0\n",
            "                  Tram          1          2   0.000101        0.5    0.00129   0.000173\n",
            "Speed: 2.0ms preprocess, 2.1ms inference, 0.0ms loss, 3.1ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/new scope model/baseline_model172\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[38;5;39m2025-06-06 18:38:58 - INFO - Starting enhanced model training...\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:58 - INFO - Starting enhanced model training...\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:58 - INFO - Starting enhanced model training...\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:58 - INFO - Starting enhanced model training...\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:58 - INFO - Starting enhanced model training...\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:38:58 - INFO - Starting enhanced model training...\u001b[0m\n",
            "INFO:YOLOComparison:Starting enhanced model training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.151 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/new scope model/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=2, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1024, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=enhanced_model11, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/new scope model, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/new scope model/enhanced_model11, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.3±0.1 ms, read: 207.7±24.5 MB/s, size: 818.4 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Samplesmall_dataset/train/labels.cache... 138 images, 54 backgrounds, 0 corrupt: 100%|██████████| 192/192 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.8±0.6 ms, read: 186.8±64.2 MB/s, size: 811.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Samplesmall_dataset/val/labels.cache... 37 images, 113 backgrounds, 0 corrupt: 100%|██████████| 150/150 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to /content/drive/MyDrive/new scope model/enhanced_model11/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/new scope model/enhanced_model11\u001b[0m\n",
            "Starting training for 2 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/2      5.19G      1.664      4.569      1.296         82       1024: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:02<00:00,  2.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        254    0.00104     0.0874    0.00101    0.00029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/2      5.21G        1.7      3.641      1.283        120       1024: 100%|██████████| 12/12 [00:06<00:00,  1.74it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        254    0.00142      0.156    0.00114   0.000302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2 epochs completed in 0.008 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/new scope model/enhanced_model11/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/drive/MyDrive/new scope model/enhanced_model11/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/drive/MyDrive/new scope model/enhanced_model11/weights/best.pt...\n",
            "Ultralytics 8.3.151 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:02<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        254    0.00141     0.0937     0.0011   0.000285\n",
            "                   Car         24        122     0.0112      0.107    0.00845    0.00218\n",
            "            Pedestrian         37         93          0          0          0          0\n",
            "                   Van          8         11          0          0          0          0\n",
            "               Cyclist          8         14          0          0          0          0\n",
            "        Person_sitting          1          1          0          0          0          0\n",
            "                  Misc          2          7   4.86e-05      0.143   4.58e-05   2.75e-05\n",
            "                 Truck          3          4          0          0          0          0\n",
            "                  Tram          1          2   9.75e-05        0.5   0.000298   7.48e-05\n",
            "Speed: 0.3ms preprocess, 2.6ms inference, 0.0ms loss, 10.2ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/new scope model/enhanced_model11\u001b[0m\n",
            "Ultralytics 8.3.151 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.3±0.1 ms, read: 329.7±76.8 MB/s, size: 840.5 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Samplesmall_dataset/val/labels.cache... 37 images, 113 backgrounds, 0 corrupt: 100%|██████████| 150/150 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:04<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150        254   0.000915     0.0738    0.00087   0.000306\n",
            "                   Car         24        122    0.00723     0.0902    0.00681    0.00239\n",
            "            Pedestrian         37         93          0          0          0          0\n",
            "                   Van          8         11          0          0          0          0\n",
            "               Cyclist          8         14          0          0          0          0\n",
            "        Person_sitting          1          1          0          0          0          0\n",
            "                  Misc          2          7          0          0          0          0\n",
            "                 Truck          3          4          0          0          0          0\n",
            "                  Tram          1          2   9.01e-05        0.5   0.000154   6.18e-05\n",
            "Speed: 2.9ms preprocess, 8.7ms inference, 0.0ms loss, 4.6ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/new scope model/enhanced_model112\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[38;5;39m2025-06-06 18:39:59 - INFO - Loading model from /content/drive/MyDrive/new scope model/enhanced_model/weights/best.pt\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:39:59 - INFO - Loading model from /content/drive/MyDrive/new scope model/enhanced_model/weights/best.pt\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:39:59 - INFO - Loading model from /content/drive/MyDrive/new scope model/enhanced_model/weights/best.pt\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:39:59 - INFO - Loading model from /content/drive/MyDrive/new scope model/enhanced_model/weights/best.pt\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:39:59 - INFO - Loading model from /content/drive/MyDrive/new scope model/enhanced_model/weights/best.pt\u001b[0m\n",
            "\u001b[38;5;39m2025-06-06 18:39:59 - INFO - Loading model from /content/drive/MyDrive/new scope model/enhanced_model/weights/best.pt\u001b[0m\n",
            "INFO:YOLOComparison:Loading model from /content/drive/MyDrive/new scope model/enhanced_model/weights/best.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n",
            "Performing prediction on 3 slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[38;5;196m2025-06-06 18:40:03 - ERROR - Error during evaluation: 'ResultsVisualizer' object has no attribute 'generate_all_visualizations'\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-39-4a600c3ee298>\", line 156, in run_complete_evaluation\n",
            "    self.visualizer.generate_all_visualizations(\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'ResultsVisualizer' object has no attribute 'generate_all_visualizations'\n",
            "\u001b[38;5;196m2025-06-06 18:40:03 - ERROR - Error during evaluation: 'ResultsVisualizer' object has no attribute 'generate_all_visualizations'\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-39-4a600c3ee298>\", line 156, in run_complete_evaluation\n",
            "    self.visualizer.generate_all_visualizations(\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'ResultsVisualizer' object has no attribute 'generate_all_visualizations'\n",
            "\u001b[38;5;196m2025-06-06 18:40:03 - ERROR - Error during evaluation: 'ResultsVisualizer' object has no attribute 'generate_all_visualizations'\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-39-4a600c3ee298>\", line 156, in run_complete_evaluation\n",
            "    self.visualizer.generate_all_visualizations(\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'ResultsVisualizer' object has no attribute 'generate_all_visualizations'\n",
            "\u001b[38;5;196m2025-06-06 18:40:03 - ERROR - Error during evaluation: 'ResultsVisualizer' object has no attribute 'generate_all_visualizations'\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-39-4a600c3ee298>\", line 156, in run_complete_evaluation\n",
            "    self.visualizer.generate_all_visualizations(\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'ResultsVisualizer' object has no attribute 'generate_all_visualizations'\n",
            "\u001b[38;5;196m2025-06-06 18:40:03 - ERROR - Error during evaluation: 'ResultsVisualizer' object has no attribute 'generate_all_visualizations'\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-39-4a600c3ee298>\", line 156, in run_complete_evaluation\n",
            "    self.visualizer.generate_all_visualizations(\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'ResultsVisualizer' object has no attribute 'generate_all_visualizations'\n",
            "\u001b[38;5;196m2025-06-06 18:40:03 - ERROR - Error during evaluation: 'ResultsVisualizer' object has no attribute 'generate_all_visualizations'\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-39-4a600c3ee298>\", line 156, in run_complete_evaluation\n",
            "    self.visualizer.generate_all_visualizations(\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'ResultsVisualizer' object has no attribute 'generate_all_visualizations'\n",
            "ERROR:YOLOComparison:Error during evaluation: 'ResultsVisualizer' object has no attribute 'generate_all_visualizations'\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-39-4a600c3ee298>\", line 156, in run_complete_evaluation\n",
            "    self.visualizer.generate_all_visualizations(\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'ResultsVisualizer' object has no attribute 'generate_all_visualizations'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 3 slices.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ResultsVisualizer' object has no attribute 'generate_all_visualizations'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-0149d9bcf6e6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Run the evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-0149d9bcf6e6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Run evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_complete_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Results saved in: {evaluator.visualizer.results_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-4a600c3ee298>\u001b[0m in \u001b[0;36mrun_complete_evaluation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# 5. Generate visualizations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             self.visualizer.generate_all_visualizations(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mresults_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ResultsVisualizer' object has no attribute 'generate_all_visualizations'"
          ]
        }
      ]
    }
  ]
}